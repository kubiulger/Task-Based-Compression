{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Balle_main_1.52.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install compressai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Cp5ycm3-F3M",
        "outputId": "1b8f3844-0990-4fff-da36-3bf6c44ac2b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: compressai in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from compressai) (0.11.1+cu111)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from compressai) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from compressai) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from compressai) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from compressai) (3.2.2)\n",
            "Requirement already satisfied: pytorch-msssim in /usr/local/lib/python3.7/dist-packages (from compressai) (0.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.1->compressai) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->compressai) (3.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->compressai) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->compressai) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->compressai) (1.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->compressai) (1.15.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->compressai) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import math\n",
        "import random\n",
        "import shutil\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "\n",
        "from compressai.datasets import ImageFolder\n",
        "from compressai.zoo import image_models\n",
        "import compressai"
      ],
      "metadata": {
        "id": "5p_DL1Ab9Qsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from compressai.zoo import (bmshj2018_factorized, bmshj2018_hyperprior, mbt2018_mean, mbt2018, cheng2020_anchor)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "metric = 'mse'  # only pre-trained model for mse are available for now\n",
        "quality = 1     # lower quality -> lower bit-rate (use lower quality to clearly see visual differences in the notebook)\n",
        "networks = {\n",
        "    'bmshj2018-factorized': bmshj2018_factorized(quality=quality, pretrained=True).eval().to(device),\n",
        "    'bmshj2018-hyperprior': bmshj2018_hyperprior(quality=quality, pretrained=True).eval().to(device),\n",
        "    'mbt2018-mean': mbt2018_mean(quality=quality, pretrained=True).eval().to(device),\n",
        "    'mbt2018': mbt2018(quality=quality, pretrained=True).eval().to(device),\n",
        "    'cheng2020-anchor': cheng2020_anchor(quality=quality, pretrained=True).eval().to(device),\n",
        "}\n",
        "\n",
        "net = networks['bmshj2018-factorized']"
      ],
      "metadata": {
        "id": "Oe6rH0dCF__N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.aux_loss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrsF-JlEKuMY",
        "outputId": "e928eb04-b1b4-449a-ae45-b22cf9cd410f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2935.3276, device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Classes"
      ],
      "metadata": {
        "id": "7TFzATzt9Yb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RateDistortionLoss(nn.Module):\n",
        "    \"\"\"Custom rate distortion loss with a Lagrangian parameter.\"\"\"\n",
        "\n",
        "    def __init__(self, lmbda=1e-2):\n",
        "        super().__init__()\n",
        "        self.crossEntropy = nn.CrossEntropyLoss()\n",
        "        self.lmbda = lmbda\n",
        "\n",
        "    def forward(self, output, target, preds, labels):\n",
        "        N, _, H, W = target.size()\n",
        "        out = {}\n",
        "        num_pixels = N * H * W\n",
        "\n",
        "        out[\"bpp_loss\"] = sum(\n",
        "            (torch.log(likelihoods).sum() / (-math.log(2) * num_pixels))\n",
        "            for likelihoods in output[\"likelihoods\"]\n",
        "        )\n",
        "        out['log_loss'] = self.crossEntropy(preds, labels)\n",
        "        out[\"loss\"] = self.lmbda * out[\"log_loss\"] + out[\"bpp_loss\"]\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class AverageMeter:\n",
        "    \"\"\"Compute running average.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "class CustomDataParallel(nn.DataParallel):\n",
        "    \"\"\"Custom DataParallel to access the module methods.\"\"\"\n",
        "\n",
        "    def __getattr__(self, key):\n",
        "        try:\n",
        "            return super().__getattr__(key)\n",
        "        except AttributeError:\n",
        "            return getattr(self.module, key)\n",
        "\n",
        "\n",
        "def configure_optimizers(net, args):\n",
        "    \"\"\"Separate parameters for the main optimizer and the auxiliary optimizer.\n",
        "    Return two optimizers\"\"\"\n",
        "\n",
        "    parameters = {\n",
        "        n\n",
        "        for n, p in net.named_parameters()\n",
        "        if not n.endswith(\".quantiles\") and p.requires_grad\n",
        "    }\n",
        "    aux_parameters = {\n",
        "        n\n",
        "        for n, p in net.named_parameters()\n",
        "        if n.endswith(\".quantiles\") and p.requires_grad\n",
        "    }\n",
        "\n",
        "    # Make sure we don't have an intersection of parameters\n",
        "    params_dict = dict(net.named_parameters())\n",
        "    inter_params = parameters & aux_parameters\n",
        "    union_params = parameters | aux_parameters\n",
        "\n",
        "    assert len(inter_params) == 0\n",
        "    assert len(union_params) - len(params_dict.keys()) == 0\n",
        "\n",
        "    optimizer = optim.Adam(\n",
        "        (params_dict[n] for n in sorted(parameters)),\n",
        "        lr=args.learning_rate\n",
        "    )\n",
        "    aux_optimizer = optim.Adam(\n",
        "        (params_dict[n] for n in sorted(aux_parameters)),\n",
        "        lr=args.aux_learning_rate,\n",
        "    )\n",
        "    return optimizer, aux_optimizer"
      ],
      "metadata": {
        "id": "0GA6VAFR9aAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Test Epochs"
      ],
      "metadata": {
        "id": "fDOq8V1t9bFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(\n",
        "    model, criterion, train_dataloader, optimizer, aux_optimizer, epoch, clip_max_norm\n",
        "):\n",
        "    model.train()\n",
        "    device = next(model.parameters()).device\n",
        "    train_acc = 0\n",
        "\n",
        "    for i, d in enumerate(train_dataloader):\n",
        "        images = d[0].to(device)\n",
        "        labels = d[1].to(device)\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        aux_optimizer.zero_grad()\n",
        "\n",
        "        if clip_max_norm > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_max_norm)\n",
        "\n",
        "        out_net = model(images)\n",
        "        preds = out_net['y_hat']\n",
        "        pred_labels = out_net['y_hat'].argmax(dim=1)\n",
        "        train_acc += torch.sum(labels == pred_labels).item()\n",
        "        out_criterion = criterion(out_net, images, preds, labels)\n",
        "        out_criterion[\"loss\"].backward()\n",
        "\n",
        "\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        aux_loss = model.aux_loss()\n",
        "        aux_loss.backward()\n",
        "        aux_optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(\n",
        "                f\"Train epoch {epoch}: [\"\n",
        "                f\"{i*len(images)}/{len(train_dataloader.dataset)}\"\n",
        "                f\" ({100. * i / len(train_dataloader):.0f}%)]\"\n",
        "\n",
        "                f'\\tLoss: {out_criterion[\"loss\"].item():.3f} |'\n",
        "                f'\\tBpp loss: {out_criterion[\"bpp_loss\"].item():.2f} |'\n",
        "                f'\\tLog loss: {out_criterion[\"log_loss\"].item():.2f} |'\n",
        "                f\"\\tAux loss: {aux_loss.item():.2f}\"\n",
        "            )\n",
        "    train_acc = train_acc/500\n",
        "    print(f'\\nTrain epoch {epoch}: \\tAcc: {train_acc:.3f} |')\n",
        "\n",
        "\n",
        "def test_epoch(epoch, test_dataloader, model, criterion):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    loss = AverageMeter()\n",
        "    bpp_loss = AverageMeter()\n",
        "    mse_loss = AverageMeter()\n",
        "    aux_loss = AverageMeter()\n",
        "    test_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for d in test_dataloader:\n",
        "            images = d[0].to(device)\n",
        "            labels = d[1].to(device)\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            out_net = model(images)\n",
        "            preds = out_net['y_hat']\n",
        "            pred_labels = out_net['y_hat'].argmax(dim=1)\n",
        "            test_acc += torch.sum(labels == pred_labels).item()\n",
        "\n",
        "            out_criterion = criterion(out_net, images, preds, labels)\n",
        "\n",
        "            aux_loss.update(model.aux_loss())\n",
        "            bpp_loss.update(out_criterion[\"bpp_loss\"])\n",
        "            loss.update(out_criterion[\"loss\"])\n",
        "    test_acc = test_acc / 100\n",
        "    print(\n",
        "        f\"Test epoch {epoch}: Average losses:\"\n",
        "        f'\\tAcc: {test_acc:.3f} |'\n",
        "        f\"\\tLoss: {loss.avg:.3f} |\"\n",
        "        f\"\\tBpp loss: {bpp_loss.avg:.2f} |\"\n",
        "        f'\\tLog loss: {out_criterion[\"log_loss\"].item():.2f} |'\n",
        "        f\"\\tAux loss: {aux_loss.avg:.2f}\\n\"\n",
        "    )\n",
        "\n",
        "    return loss.avg\n",
        "\n",
        "\n",
        "def save_checkpoint(state, epoch, is_best, filename, best_filename):\n",
        "    torch.save(state, str(epoch)+filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(str(epoch)+ filename, best_filename)\n"
      ],
      "metadata": {
        "id": "tsS_geUH9hpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "metadata": {
        "id": "0XaiYtqC9nLo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHYmZp2I9DVu"
      },
      "outputs": [],
      "source": [
        "def main(model, num_workers, batch_size, cuda, epoch, patch_size, learning_rate, lmbda):\n",
        "    # args = parse_args(argv)\n",
        "\n",
        "    # if args.seed is not None:\n",
        "    #     torch.manual_seed(args.seed)\n",
        "    #     random.seed(args.seed)\n",
        "\n",
        "    class arguments:\n",
        "      def __init__(self, model, num_workers, batch_size, cuda, epoch, patch_size, learning_rate, lmbda):\n",
        "        self.model = model\n",
        "        self.num_workers = num_workers\n",
        "        self.batch_size = batch_size\n",
        "        self.test_batch_size = 100\n",
        "        self.cuda = cuda\n",
        "        self.epochs = epoch\n",
        "        self.patch_size = patch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.aux_learning_rate = learning_rate\n",
        "        self.lmbda = lmbda\n",
        "        self.save = True\n",
        "        self.seed = False\n",
        "        self.clip_max_norm = 1.0\n",
        "        self.checkpoint = False\n",
        "\n",
        "\n",
        "    tr_mean = np.asarray([0.4914, 0.4822, 0.4465])\n",
        "    tr_std = np.asarray([0.247, 0.243, 0.261])\n",
        "\n",
        "    args = arguments(model, num_workers, batch_size, cuda, epoch, patch_size, learning_rate, lmbda)\n",
        "\n",
        "    train_transforms = transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.RandomCrop(args.patch_size), torchvision.transforms.Normalize(tr_mean, tr_std)]\n",
        "    )\n",
        "\n",
        "    test_transforms = transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.CenterCrop(args.patch_size), torchvision.transforms.Normalize(tr_mean, tr_std)]\n",
        "    )\n",
        "\n",
        "    # train_dataset = ImageFolder(args.dataset, split=\"train\", transform=train_transforms)\n",
        "    # test_dataset = ImageFolder(args.dataset, split=\"test\", transform=test_transforms)\n",
        "\n",
        "    train_dataset = torchvision.datasets.CIFAR10('./CIFAR-10/',train=True,download=True, transform=train_transforms)\n",
        "    test_dataset = torchvision.datasets.CIFAR10('./CIFAR-10/',train=False,download=True, transform=test_transforms)\n",
        "\n",
        "    device = \"cuda\" if args.cuda and torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.num_workers,\n",
        "        shuffle=True,\n",
        "        pin_memory=(device == \"cuda\"),\n",
        "    )\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=args.test_batch_size,\n",
        "        num_workers=args.num_workers,\n",
        "        shuffle=False,\n",
        "        pin_memory=(device == \"cuda\"),\n",
        "    )\n",
        "\n",
        "    net_a = model.g_a \n",
        "    net_e = model.entropy_bottleneck\n",
        "    net_s = model.g_s\n",
        "    resnet = torchvision.models.resnet18(pretrained = True)\n",
        "    resnet.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
        "\n",
        "    class Net(nn.Module):\n",
        "        def __init__(self, resnet, net_a, net_e, net_s):\n",
        "            super(Net, self).__init__()\n",
        "\n",
        "            self.a = net_a\n",
        "            self.e = net_e\n",
        "            self.s = net_s\n",
        "            self.res = resnet\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.a(x)\n",
        "            x_hat, likelihoods = self.e(x)\n",
        "            x_hat = self.s(x_hat)\n",
        "            y_hat = self.res(x_hat)\n",
        "\n",
        "            return {\n",
        "                    \"y_hat\": y_hat,\n",
        "                    \"likelihoods\": likelihoods}\n",
        "\n",
        "\n",
        "    net = Net(resnet, net_a, net_e, net_s)\n",
        "    net.aux_loss = model.aux_loss\n",
        "    net = net.to(device)\n",
        "\n",
        "    if args.cuda and torch.cuda.device_count() > 1:\n",
        "        net = CustomDataParallel(net)\n",
        "\n",
        "    optimizer, aux_optimizer = configure_optimizers(net, args)\n",
        "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\",factor=0.5)\n",
        "    criterion = RateDistortionLoss(lmbda=args.lmbda)\n",
        "\n",
        "    filename = str(args.lmbda) + '_check.pth.tar'\n",
        "    best_filename = 'best' + filename\n",
        "\n",
        "    last_epoch = 0\n",
        "    if args.checkpoint:  # load from previous checkpoint\n",
        "        print(\"Loading\", args.checkpoint)\n",
        "        checkpoint = torch.load(args.checkpoint, map_location=device)\n",
        "        last_epoch = checkpoint[\"epoch\"] + 1\n",
        "        net.load_state_dict(checkpoint[\"state_dict\"])\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "        aux_optimizer.load_state_dict(checkpoint[\"aux_optimizer\"])\n",
        "        lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n",
        "\n",
        "    best_loss = float(\"inf\")\n",
        "    for epoch in range(last_epoch, args.epochs):\n",
        "        T11 = time.time()\n",
        "\n",
        "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "        train_one_epoch(\n",
        "            net,\n",
        "            criterion,\n",
        "            train_dataloader,\n",
        "            optimizer,\n",
        "            aux_optimizer,\n",
        "            epoch,\n",
        "            args.clip_max_norm,\n",
        "        )\n",
        "        loss = test_epoch(epoch, test_dataloader, net, criterion)\n",
        "        lr_scheduler.step(loss)\n",
        "        T22 = time.time()\n",
        "        print(f\"Time: {T22-T11:.4f}\")\n",
        "        is_best = loss < best_loss\n",
        "        best_loss = min(loss, best_loss)\n",
        "\n",
        "        if args.save:\n",
        "            save_checkpoint(\n",
        "                {\n",
        "                    \"epoch\": epoch,\n",
        "                    \"state_dict\": net.state_dict(),\n",
        "                    \"loss\": loss,\n",
        "                    \"optimizer\": optimizer.state_dict(),\n",
        "                    \"aux_optimizer\": aux_optimizer.state_dict(),\n",
        "                    \"lr_scheduler\": lr_scheduler.state_dict(),\n",
        "                },\n",
        "                epoch,\n",
        "                is_best,\n",
        "                filename,\n",
        "                best_filename,\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net_out = main(net, 2, 64, 1, 200, 32, 0.001, 0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cXEv88vFmu7",
        "outputId": "3d15d326-9310-4469-a8c4-7da27255de5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Learning rate: 0.001\n",
            "Train epoch 0: [0/50000 (0%)]\tLoss: 2.650 |\tBpp loss: 0.53 |\tLog loss: 2.35 |\tAux loss: 2970.16\n",
            "Train epoch 0: [6400/50000 (13%)]\tLoss: 2.299 |\tBpp loss: 0.46 |\tLog loss: 2.04 |\tAux loss: 2869.53\n",
            "Train epoch 0: [12800/50000 (26%)]\tLoss: 2.274 |\tBpp loss: 0.41 |\tLog loss: 2.08 |\tAux loss: 2607.65\n",
            "Train epoch 0: [19200/50000 (38%)]\tLoss: 2.320 |\tBpp loss: 0.34 |\tLog loss: 2.20 |\tAux loss: 2094.23\n",
            "Train epoch 0: [25600/50000 (51%)]\tLoss: 2.151 |\tBpp loss: 0.35 |\tLog loss: 2.00 |\tAux loss: 2013.14\n",
            "Train epoch 0: [32000/50000 (64%)]\tLoss: 2.156 |\tBpp loss: 0.29 |\tLog loss: 2.07 |\tAux loss: 1808.46\n",
            "Train epoch 0: [38400/50000 (77%)]\tLoss: 1.976 |\tBpp loss: 0.27 |\tLog loss: 1.89 |\tAux loss: 1602.67\n",
            "Train epoch 0: [44800/50000 (90%)]\tLoss: 2.111 |\tBpp loss: 0.26 |\tLog loss: 2.06 |\tAux loss: 1484.24\n",
            "\n",
            "Train epoch 0: \tAcc: 22.160 |\n",
            "Test epoch 0: Average losses:\tAcc: 27.380 |\tLoss: 1.772 |\tBpp loss: 0.06 |\tLog loss: 1.83 |\tAux loss: 1434.10\n",
            "\n",
            "Time: 62.6607\n",
            "Learning rate: 0.001\n",
            "Train epoch 1: [0/50000 (0%)]\tLoss: 1.852 |\tBpp loss: 0.24 |\tLog loss: 1.79 |\tAux loss: 1432.68\n",
            "Train epoch 1: [6400/50000 (13%)]\tLoss: 2.084 |\tBpp loss: 0.21 |\tLog loss: 2.08 |\tAux loss: 1391.60\n",
            "Train epoch 1: [12800/50000 (26%)]\tLoss: 1.929 |\tBpp loss: 0.21 |\tLog loss: 1.91 |\tAux loss: 1341.75\n",
            "Train epoch 1: [19200/50000 (38%)]\tLoss: 1.826 |\tBpp loss: 0.20 |\tLog loss: 1.81 |\tAux loss: 1226.28\n",
            "Train epoch 1: [25600/50000 (51%)]\tLoss: 1.843 |\tBpp loss: 0.18 |\tLog loss: 1.85 |\tAux loss: 1277.26\n",
            "Train epoch 1: [32000/50000 (64%)]\tLoss: 1.909 |\tBpp loss: 0.18 |\tLog loss: 1.92 |\tAux loss: 1104.53\n",
            "Train epoch 1: [38400/50000 (77%)]\tLoss: 1.751 |\tBpp loss: 0.15 |\tLog loss: 1.78 |\tAux loss: 1116.20\n",
            "Train epoch 1: [44800/50000 (90%)]\tLoss: 1.753 |\tBpp loss: 0.15 |\tLog loss: 1.78 |\tAux loss: 1108.96\n",
            "\n",
            "Train epoch 1: \tAcc: 31.690 |\n",
            "Test epoch 1: Average losses:\tAcc: 31.930 |\tLoss: 1.651 |\tBpp loss: 0.05 |\tLog loss: 1.76 |\tAux loss: 1159.06\n",
            "\n",
            "Time: 62.0069\n",
            "Learning rate: 0.001\n",
            "Train epoch 2: [0/50000 (0%)]\tLoss: 1.754 |\tBpp loss: 0.13 |\tLog loss: 1.80 |\tAux loss: 1161.20\n",
            "Train epoch 2: [6400/50000 (13%)]\tLoss: 1.520 |\tBpp loss: 0.13 |\tLog loss: 1.55 |\tAux loss: 1239.85\n",
            "Train epoch 2: [12800/50000 (26%)]\tLoss: 1.839 |\tBpp loss: 0.14 |\tLog loss: 1.89 |\tAux loss: 1176.93\n",
            "Train epoch 2: [19200/50000 (38%)]\tLoss: 1.564 |\tBpp loss: 0.11 |\tLog loss: 1.61 |\tAux loss: 1117.84\n",
            "Train epoch 2: [25600/50000 (51%)]\tLoss: 1.571 |\tBpp loss: 0.09 |\tLog loss: 1.65 |\tAux loss: 1225.79\n",
            "Train epoch 2: [32000/50000 (64%)]\tLoss: 1.681 |\tBpp loss: 0.10 |\tLog loss: 1.76 |\tAux loss: 1292.18\n",
            "Train epoch 2: [38400/50000 (77%)]\tLoss: 1.625 |\tBpp loss: 0.09 |\tLog loss: 1.70 |\tAux loss: 1331.70\n",
            "Train epoch 2: [44800/50000 (90%)]\tLoss: 1.687 |\tBpp loss: 0.08 |\tLog loss: 1.78 |\tAux loss: 1318.21\n",
            "\n",
            "Train epoch 2: \tAcc: 36.924 |\n",
            "Test epoch 2: Average losses:\tAcc: 38.090 |\tLoss: 1.526 |\tBpp loss: 0.05 |\tLog loss: 1.53 |\tAux loss: 1347.89\n",
            "\n",
            "Time: 62.7544\n",
            "Learning rate: 0.001\n",
            "Train epoch 3: [0/50000 (0%)]\tLoss: 1.479 |\tBpp loss: 0.08 |\tLog loss: 1.56 |\tAux loss: 1348.80\n",
            "Train epoch 3: [6400/50000 (13%)]\tLoss: 1.572 |\tBpp loss: 0.09 |\tLog loss: 1.65 |\tAux loss: 1327.22\n",
            "Train epoch 3: [12800/50000 (26%)]\tLoss: 1.437 |\tBpp loss: 0.09 |\tLog loss: 1.49 |\tAux loss: 1148.72\n",
            "Train epoch 3: [19200/50000 (38%)]\tLoss: 1.652 |\tBpp loss: 0.09 |\tLog loss: 1.74 |\tAux loss: 1208.84\n",
            "Train epoch 3: [25600/50000 (51%)]\tLoss: 1.508 |\tBpp loss: 0.08 |\tLog loss: 1.58 |\tAux loss: 1297.87\n",
            "Train epoch 3: [32000/50000 (64%)]\tLoss: 1.366 |\tBpp loss: 0.08 |\tLog loss: 1.43 |\tAux loss: 1388.25\n",
            "Train epoch 3: [38400/50000 (77%)]\tLoss: 1.464 |\tBpp loss: 0.08 |\tLog loss: 1.54 |\tAux loss: 1380.51\n",
            "Train epoch 3: [44800/50000 (90%)]\tLoss: 1.591 |\tBpp loss: 0.08 |\tLog loss: 1.68 |\tAux loss: 1447.70\n",
            "\n",
            "Train epoch 3: \tAcc: 42.360 |\n",
            "Test epoch 3: Average losses:\tAcc: 43.830 |\tLoss: 1.385 |\tBpp loss: 0.05 |\tLog loss: 1.41 |\tAux loss: 1448.81\n",
            "\n",
            "Time: 63.1189\n",
            "Learning rate: 0.001\n",
            "Train epoch 4: [0/50000 (0%)]\tLoss: 1.571 |\tBpp loss: 0.08 |\tLog loss: 1.65 |\tAux loss: 1449.35\n",
            "Train epoch 4: [6400/50000 (13%)]\tLoss: 1.478 |\tBpp loss: 0.08 |\tLog loss: 1.55 |\tAux loss: 1478.73\n",
            "Train epoch 4: [12800/50000 (26%)]\tLoss: 1.400 |\tBpp loss: 0.08 |\tLog loss: 1.47 |\tAux loss: 1459.43\n",
            "Train epoch 4: [19200/50000 (38%)]\tLoss: 1.330 |\tBpp loss: 0.09 |\tLog loss: 1.38 |\tAux loss: 1502.57\n",
            "Train epoch 4: [25600/50000 (51%)]\tLoss: 1.162 |\tBpp loss: 0.08 |\tLog loss: 1.20 |\tAux loss: 1519.62\n",
            "Train epoch 4: [32000/50000 (64%)]\tLoss: 1.153 |\tBpp loss: 0.08 |\tLog loss: 1.20 |\tAux loss: 1538.44\n",
            "Train epoch 4: [38400/50000 (77%)]\tLoss: 1.287 |\tBpp loss: 0.08 |\tLog loss: 1.34 |\tAux loss: 1497.56\n",
            "Train epoch 4: [44800/50000 (90%)]\tLoss: 1.314 |\tBpp loss: 0.08 |\tLog loss: 1.37 |\tAux loss: 1591.99\n",
            "\n",
            "Train epoch 4: \tAcc: 47.296 |\n",
            "Test epoch 4: Average losses:\tAcc: 46.800 |\tLoss: 1.340 |\tBpp loss: 0.05 |\tLog loss: 1.30 |\tAux loss: 1610.04\n",
            "\n",
            "Time: 62.7060\n",
            "Learning rate: 0.001\n",
            "Train epoch 5: [0/50000 (0%)]\tLoss: 1.202 |\tBpp loss: 0.08 |\tLog loss: 1.25 |\tAux loss: 1610.98\n",
            "Train epoch 5: [6400/50000 (13%)]\tLoss: 1.277 |\tBpp loss: 0.09 |\tLog loss: 1.32 |\tAux loss: 1290.98\n",
            "Train epoch 5: [12800/50000 (26%)]\tLoss: 1.434 |\tBpp loss: 0.09 |\tLog loss: 1.50 |\tAux loss: 1310.29\n",
            "Train epoch 5: [19200/50000 (38%)]\tLoss: 2.124 |\tBpp loss: 0.13 |\tLog loss: 2.21 |\tAux loss: 2302.77\n",
            "Train epoch 5: [25600/50000 (51%)]\tLoss: 1.695 |\tBpp loss: 0.11 |\tLog loss: 1.77 |\tAux loss: 2088.60\n",
            "Train epoch 5: [32000/50000 (64%)]\tLoss: 1.486 |\tBpp loss: 0.09 |\tLog loss: 1.55 |\tAux loss: 1820.78\n",
            "Train epoch 5: [38400/50000 (77%)]\tLoss: 1.839 |\tBpp loss: 0.09 |\tLog loss: 1.94 |\tAux loss: 1583.93\n",
            "Train epoch 5: [44800/50000 (90%)]\tLoss: 1.323 |\tBpp loss: 0.09 |\tLog loss: 1.37 |\tAux loss: 1455.34\n",
            "\n",
            "Train epoch 5: \tAcc: 39.194 |\n",
            "Test epoch 5: Average losses:\tAcc: 46.090 |\tLoss: 1.449 |\tBpp loss: 0.05 |\tLog loss: 1.38 |\tAux loss: 1416.11\n",
            "\n",
            "Time: 62.8705\n",
            "Learning rate: 0.001\n",
            "Train epoch 6: [0/50000 (0%)]\tLoss: 1.286 |\tBpp loss: 0.08 |\tLog loss: 1.34 |\tAux loss: 1417.49\n",
            "Train epoch 6: [6400/50000 (13%)]\tLoss: 1.574 |\tBpp loss: 0.08 |\tLog loss: 1.66 |\tAux loss: 1402.57\n",
            "Train epoch 6: [12800/50000 (26%)]\tLoss: 1.427 |\tBpp loss: 0.08 |\tLog loss: 1.50 |\tAux loss: 1420.69\n",
            "Train epoch 6: [19200/50000 (38%)]\tLoss: 1.181 |\tBpp loss: 0.08 |\tLog loss: 1.23 |\tAux loss: 1453.78\n",
            "Train epoch 6: [25600/50000 (51%)]\tLoss: 1.247 |\tBpp loss: 0.08 |\tLog loss: 1.30 |\tAux loss: 1448.98\n",
            "Train epoch 6: [32000/50000 (64%)]\tLoss: 1.440 |\tBpp loss: 0.08 |\tLog loss: 1.51 |\tAux loss: 1414.63\n",
            "Train epoch 6: [38400/50000 (77%)]\tLoss: 1.206 |\tBpp loss: 0.08 |\tLog loss: 1.25 |\tAux loss: 1405.56\n",
            "Train epoch 6: [44800/50000 (90%)]\tLoss: 1.266 |\tBpp loss: 0.08 |\tLog loss: 1.32 |\tAux loss: 1452.65\n",
            "\n",
            "Train epoch 6: \tAcc: 49.602 |\n",
            "Test epoch 6: Average losses:\tAcc: 50.720 |\tLoss: 1.247 |\tBpp loss: 0.05 |\tLog loss: 1.22 |\tAux loss: 1488.18\n",
            "\n",
            "Time: 63.3040\n",
            "Learning rate: 0.001\n",
            "Train epoch 7: [0/50000 (0%)]\tLoss: 1.125 |\tBpp loss: 0.08 |\tLog loss: 1.16 |\tAux loss: 1490.12\n",
            "Train epoch 7: [6400/50000 (13%)]\tLoss: 1.363 |\tBpp loss: 0.08 |\tLog loss: 1.42 |\tAux loss: 1489.96\n",
            "Train epoch 7: [12800/50000 (26%)]\tLoss: 1.452 |\tBpp loss: 0.08 |\tLog loss: 1.53 |\tAux loss: 1485.56\n",
            "Train epoch 7: [19200/50000 (38%)]\tLoss: 1.208 |\tBpp loss: 0.08 |\tLog loss: 1.25 |\tAux loss: 1492.22\n",
            "Train epoch 7: [25600/50000 (51%)]\tLoss: 1.277 |\tBpp loss: 0.08 |\tLog loss: 1.33 |\tAux loss: 1470.76\n",
            "Train epoch 7: [32000/50000 (64%)]\tLoss: 1.260 |\tBpp loss: 0.09 |\tLog loss: 1.30 |\tAux loss: 1505.65\n",
            "Train epoch 7: [38400/50000 (77%)]\tLoss: 1.203 |\tBpp loss: 0.08 |\tLog loss: 1.24 |\tAux loss: 1646.36\n",
            "Train epoch 7: [44800/50000 (90%)]\tLoss: 1.675 |\tBpp loss: 0.09 |\tLog loss: 1.77 |\tAux loss: 1423.31\n",
            "\n",
            "Train epoch 7: \tAcc: 51.038 |\n",
            "Test epoch 7: Average losses:\tAcc: 31.000 |\tLoss: 2.661 |\tBpp loss: 0.10 |\tLog loss: 7.67 |\tAux loss: 2328.33\n",
            "\n",
            "Time: 63.1216\n",
            "Learning rate: 0.001\n",
            "Train epoch 8: [0/50000 (0%)]\tLoss: 2.010 |\tBpp loss: 0.19 |\tLog loss: 2.02 |\tAux loss: 2376.28\n",
            "Train epoch 8: [6400/50000 (13%)]\tLoss: 1.541 |\tBpp loss: 0.10 |\tLog loss: 1.60 |\tAux loss: 3424.92\n",
            "Train epoch 8: [12800/50000 (26%)]\tLoss: 1.367 |\tBpp loss: 0.10 |\tLog loss: 1.41 |\tAux loss: 2947.06\n",
            "Train epoch 8: [19200/50000 (38%)]\tLoss: 1.136 |\tBpp loss: 0.10 |\tLog loss: 1.15 |\tAux loss: 2456.83\n",
            "Train epoch 8: [25600/50000 (51%)]\tLoss: 1.073 |\tBpp loss: 0.09 |\tLog loss: 1.09 |\tAux loss: 2057.39\n",
            "Train epoch 8: [32000/50000 (64%)]\tLoss: 1.083 |\tBpp loss: 0.09 |\tLog loss: 1.10 |\tAux loss: 1845.36\n",
            "Train epoch 8: [38400/50000 (77%)]\tLoss: 1.197 |\tBpp loss: 0.09 |\tLog loss: 1.23 |\tAux loss: 1578.02\n",
            "Train epoch 8: [44800/50000 (90%)]\tLoss: 1.251 |\tBpp loss: 0.09 |\tLog loss: 1.29 |\tAux loss: 1435.58\n",
            "\n",
            "Train epoch 8: \tAcc: 51.054 |\n",
            "Test epoch 8: Average losses:\tAcc: 53.880 |\tLoss: 1.186 |\tBpp loss: 0.05 |\tLog loss: 1.22 |\tAux loss: 1404.76\n",
            "\n",
            "Time: 63.0805\n",
            "Learning rate: 0.001\n",
            "Train epoch 9: [0/50000 (0%)]\tLoss: 1.072 |\tBpp loss: 0.09 |\tLog loss: 1.10 |\tAux loss: 1408.44\n",
            "Train epoch 9: [6400/50000 (13%)]\tLoss: 1.197 |\tBpp loss: 0.09 |\tLog loss: 1.23 |\tAux loss: 1330.01\n",
            "Train epoch 9: [12800/50000 (26%)]\tLoss: 1.168 |\tBpp loss: 0.08 |\tLog loss: 1.20 |\tAux loss: 1318.12\n",
            "Train epoch 9: [19200/50000 (38%)]\tLoss: 1.354 |\tBpp loss: 0.08 |\tLog loss: 1.41 |\tAux loss: 1324.73\n",
            "Train epoch 9: [25600/50000 (51%)]\tLoss: 1.083 |\tBpp loss: 0.08 |\tLog loss: 1.11 |\tAux loss: 1263.32\n",
            "Train epoch 9: [32000/50000 (64%)]\tLoss: 1.912 |\tBpp loss: 0.19 |\tLog loss: 1.91 |\tAux loss: 1531.67\n",
            "Train epoch 9: [38400/50000 (77%)]\tLoss: 1.475 |\tBpp loss: 0.11 |\tLog loss: 1.52 |\tAux loss: 3191.25\n",
            "Train epoch 9: [44800/50000 (90%)]\tLoss: 1.476 |\tBpp loss: 0.09 |\tLog loss: 1.54 |\tAux loss: 2726.12\n",
            "\n",
            "Train epoch 9: \tAcc: 51.902 |\n",
            "Test epoch 9: Average losses:\tAcc: 51.590 |\tLoss: 1.237 |\tBpp loss: 0.05 |\tLog loss: 1.29 |\tAux loss: 2171.08\n",
            "\n",
            "Time: 63.1901\n",
            "Learning rate: 0.001\n",
            "Train epoch 10: [0/50000 (0%)]\tLoss: 1.108 |\tBpp loss: 0.09 |\tLog loss: 1.13 |\tAux loss: 2169.09\n",
            "Train epoch 10: [6400/50000 (13%)]\tLoss: 1.037 |\tBpp loss: 0.09 |\tLog loss: 1.05 |\tAux loss: 1630.01\n",
            "Train epoch 10: [12800/50000 (26%)]\tLoss: 1.065 |\tBpp loss: 0.09 |\tLog loss: 1.09 |\tAux loss: 1231.87\n",
            "Train epoch 10: [19200/50000 (38%)]\tLoss: 1.122 |\tBpp loss: 0.09 |\tLog loss: 1.14 |\tAux loss: 1071.52\n",
            "Train epoch 10: [25600/50000 (51%)]\tLoss: 1.186 |\tBpp loss: 0.09 |\tLog loss: 1.22 |\tAux loss: 1076.98\n",
            "Train epoch 10: [32000/50000 (64%)]\tLoss: 1.155 |\tBpp loss: 0.09 |\tLog loss: 1.19 |\tAux loss: 1107.73\n",
            "Train epoch 10: [38400/50000 (77%)]\tLoss: 0.976 |\tBpp loss: 0.09 |\tLog loss: 0.98 |\tAux loss: 1087.37\n",
            "Train epoch 10: [44800/50000 (90%)]\tLoss: 1.150 |\tBpp loss: 0.09 |\tLog loss: 1.18 |\tAux loss: 1079.01\n",
            "\n",
            "Train epoch 10: \tAcc: 56.524 |\n",
            "Test epoch 10: Average losses:\tAcc: 55.770 |\tLoss: 1.122 |\tBpp loss: 0.05 |\tLog loss: 1.20 |\tAux loss: 1094.80\n",
            "\n",
            "Time: 63.0061\n",
            "Learning rate: 0.001\n",
            "Train epoch 11: [0/50000 (0%)]\tLoss: 1.431 |\tBpp loss: 0.08 |\tLog loss: 1.50 |\tAux loss: 1094.82\n",
            "Train epoch 11: [6400/50000 (13%)]\tLoss: 0.863 |\tBpp loss: 0.09 |\tLog loss: 0.86 |\tAux loss: 1050.05\n",
            "Train epoch 11: [12800/50000 (26%)]\tLoss: 1.059 |\tBpp loss: 0.09 |\tLog loss: 1.08 |\tAux loss: 1028.14\n",
            "Train epoch 11: [19200/50000 (38%)]\tLoss: 1.328 |\tBpp loss: 0.09 |\tLog loss: 1.38 |\tAux loss: 1037.31\n",
            "Train epoch 11: [25600/50000 (51%)]\tLoss: 1.137 |\tBpp loss: 0.09 |\tLog loss: 1.17 |\tAux loss: 993.91\n",
            "Train epoch 11: [32000/50000 (64%)]\tLoss: 1.049 |\tBpp loss: 0.09 |\tLog loss: 1.07 |\tAux loss: 1031.22\n",
            "Train epoch 11: [38400/50000 (77%)]\tLoss: 1.233 |\tBpp loss: 0.09 |\tLog loss: 1.27 |\tAux loss: 978.15\n",
            "Train epoch 11: [44800/50000 (90%)]\tLoss: 1.094 |\tBpp loss: 0.09 |\tLog loss: 1.12 |\tAux loss: 959.55\n",
            "\n",
            "Train epoch 11: \tAcc: 59.606 |\n",
            "Test epoch 11: Average losses:\tAcc: 58.130 |\tLoss: 1.096 |\tBpp loss: 0.05 |\tLog loss: 1.18 |\tAux loss: 972.01\n",
            "\n",
            "Time: 63.2548\n",
            "Learning rate: 0.001\n",
            "Train epoch 12: [0/50000 (0%)]\tLoss: 1.217 |\tBpp loss: 0.09 |\tLog loss: 1.26 |\tAux loss: 973.44\n",
            "Train epoch 12: [6400/50000 (13%)]\tLoss: 1.122 |\tBpp loss: 0.10 |\tLog loss: 1.14 |\tAux loss: 1278.53\n",
            "Train epoch 12: [12800/50000 (26%)]\tLoss: 1.205 |\tBpp loss: 0.09 |\tLog loss: 1.24 |\tAux loss: 927.75\n",
            "Train epoch 12: [19200/50000 (38%)]\tLoss: 1.106 |\tBpp loss: 0.09 |\tLog loss: 1.13 |\tAux loss: 926.54\n",
            "Train epoch 12: [25600/50000 (51%)]\tLoss: 1.009 |\tBpp loss: 0.09 |\tLog loss: 1.02 |\tAux loss: 882.55\n",
            "Train epoch 12: [32000/50000 (64%)]\tLoss: 1.152 |\tBpp loss: 0.10 |\tLog loss: 1.17 |\tAux loss: 883.52\n",
            "Train epoch 12: [38400/50000 (77%)]\tLoss: 1.318 |\tBpp loss: 0.10 |\tLog loss: 1.36 |\tAux loss: 930.53\n",
            "Train epoch 12: [44800/50000 (90%)]\tLoss: 0.832 |\tBpp loss: 0.09 |\tLog loss: 0.82 |\tAux loss: 751.92\n",
            "\n",
            "Train epoch 12: \tAcc: 58.200 |\n",
            "Test epoch 12: Average losses:\tAcc: 56.930 |\tLoss: 1.107 |\tBpp loss: 0.05 |\tLog loss: 1.21 |\tAux loss: 789.06\n",
            "\n",
            "Time: 62.6316\n",
            "Learning rate: 0.001\n",
            "Train epoch 13: [0/50000 (0%)]\tLoss: 0.916 |\tBpp loss: 0.09 |\tLog loss: 0.92 |\tAux loss: 790.87\n",
            "Train epoch 13: [6400/50000 (13%)]\tLoss: 1.074 |\tBpp loss: 0.09 |\tLog loss: 1.09 |\tAux loss: 798.52\n",
            "Train epoch 13: [12800/50000 (26%)]\tLoss: 1.057 |\tBpp loss: 0.09 |\tLog loss: 1.08 |\tAux loss: 836.73\n",
            "Train epoch 13: [19200/50000 (38%)]\tLoss: 0.890 |\tBpp loss: 0.09 |\tLog loss: 0.89 |\tAux loss: 816.44\n",
            "Train epoch 13: [25600/50000 (51%)]\tLoss: 1.140 |\tBpp loss: 0.09 |\tLog loss: 1.17 |\tAux loss: 809.34\n",
            "Train epoch 13: [32000/50000 (64%)]\tLoss: 1.158 |\tBpp loss: 0.09 |\tLog loss: 1.19 |\tAux loss: 809.16\n",
            "Train epoch 13: [38400/50000 (77%)]\tLoss: 1.074 |\tBpp loss: 0.09 |\tLog loss: 1.09 |\tAux loss: 800.87\n",
            "Train epoch 13: [44800/50000 (90%)]\tLoss: 1.109 |\tBpp loss: 0.09 |\tLog loss: 1.13 |\tAux loss: 788.44\n",
            "\n",
            "Train epoch 13: \tAcc: 61.912 |\n",
            "Test epoch 13: Average losses:\tAcc: 60.080 |\tLoss: 1.047 |\tBpp loss: 0.05 |\tLog loss: 1.11 |\tAux loss: 795.26\n",
            "\n",
            "Time: 63.3884\n",
            "Learning rate: 0.001\n",
            "Train epoch 14: [0/50000 (0%)]\tLoss: 1.150 |\tBpp loss: 0.09 |\tLog loss: 1.18 |\tAux loss: 797.40\n",
            "Train epoch 14: [6400/50000 (13%)]\tLoss: 1.021 |\tBpp loss: 0.09 |\tLog loss: 1.03 |\tAux loss: 785.91\n",
            "Train epoch 14: [12800/50000 (26%)]\tLoss: 1.032 |\tBpp loss: 0.09 |\tLog loss: 1.05 |\tAux loss: 763.67\n",
            "Train epoch 14: [19200/50000 (38%)]\tLoss: 1.154 |\tBpp loss: 0.09 |\tLog loss: 1.19 |\tAux loss: 762.36\n",
            "Train epoch 14: [25600/50000 (51%)]\tLoss: 0.819 |\tBpp loss: 0.09 |\tLog loss: 0.81 |\tAux loss: 747.83\n",
            "Train epoch 14: [32000/50000 (64%)]\tLoss: 0.970 |\tBpp loss: 0.09 |\tLog loss: 0.98 |\tAux loss: 730.76\n",
            "Train epoch 14: [38400/50000 (77%)]\tLoss: 0.972 |\tBpp loss: 0.09 |\tLog loss: 0.98 |\tAux loss: 660.61\n",
            "Train epoch 14: [44800/50000 (90%)]\tLoss: 0.851 |\tBpp loss: 0.09 |\tLog loss: 0.84 |\tAux loss: 673.30\n",
            "\n",
            "Train epoch 14: \tAcc: 63.052 |\n",
            "Test epoch 14: Average losses:\tAcc: 62.210 |\tLoss: 1.051 |\tBpp loss: 0.05 |\tLog loss: 0.96 |\tAux loss: 688.61\n",
            "\n",
            "Time: 62.4431\n",
            "Learning rate: 0.001\n",
            "Train epoch 15: [0/50000 (0%)]\tLoss: 0.909 |\tBpp loss: 0.09 |\tLog loss: 0.91 |\tAux loss: 688.78\n",
            "Train epoch 15: [6400/50000 (13%)]\tLoss: 0.981 |\tBpp loss: 0.09 |\tLog loss: 0.99 |\tAux loss: 675.99\n",
            "Train epoch 15: [12800/50000 (26%)]\tLoss: 1.091 |\tBpp loss: 0.09 |\tLog loss: 1.11 |\tAux loss: 668.94\n",
            "Train epoch 15: [19200/50000 (38%)]\tLoss: 1.118 |\tBpp loss: 0.11 |\tLog loss: 1.12 |\tAux loss: 704.47\n",
            "Train epoch 15: [25600/50000 (51%)]\tLoss: 1.147 |\tBpp loss: 0.10 |\tLog loss: 1.16 |\tAux loss: 603.03\n",
            "Train epoch 15: [32000/50000 (64%)]\tLoss: 1.272 |\tBpp loss: 0.12 |\tLog loss: 1.28 |\tAux loss: 1320.25\n",
            "Train epoch 15: [38400/50000 (77%)]\tLoss: 1.106 |\tBpp loss: 0.11 |\tLog loss: 1.11 |\tAux loss: 623.53\n",
            "Train epoch 15: [44800/50000 (90%)]\tLoss: 1.032 |\tBpp loss: 0.09 |\tLog loss: 1.04 |\tAux loss: 578.97\n",
            "\n",
            "Train epoch 15: \tAcc: 62.146 |\n",
            "Test epoch 15: Average losses:\tAcc: 60.780 |\tLoss: 1.035 |\tBpp loss: 0.05 |\tLog loss: 1.03 |\tAux loss: 589.54\n",
            "\n",
            "Time: 62.8317\n",
            "Learning rate: 0.001\n",
            "Train epoch 16: [0/50000 (0%)]\tLoss: 0.976 |\tBpp loss: 0.09 |\tLog loss: 0.98 |\tAux loss: 589.91\n",
            "Train epoch 16: [6400/50000 (13%)]\tLoss: 0.793 |\tBpp loss: 0.09 |\tLog loss: 0.78 |\tAux loss: 566.14\n",
            "Train epoch 16: [12800/50000 (26%)]\tLoss: 0.994 |\tBpp loss: 0.10 |\tLog loss: 1.00 |\tAux loss: 571.56\n",
            "Train epoch 16: [19200/50000 (38%)]\tLoss: 1.055 |\tBpp loss: 0.10 |\tLog loss: 1.07 |\tAux loss: 548.31\n",
            "Train epoch 16: [25600/50000 (51%)]\tLoss: 1.006 |\tBpp loss: 0.09 |\tLog loss: 1.01 |\tAux loss: 548.45\n",
            "Train epoch 16: [32000/50000 (64%)]\tLoss: 0.830 |\tBpp loss: 0.09 |\tLog loss: 0.82 |\tAux loss: 552.67\n",
            "Train epoch 16: [38400/50000 (77%)]\tLoss: 0.987 |\tBpp loss: 0.09 |\tLog loss: 1.00 |\tAux loss: 528.11\n",
            "Train epoch 16: [44800/50000 (90%)]\tLoss: 0.978 |\tBpp loss: 0.09 |\tLog loss: 0.98 |\tAux loss: 540.07\n",
            "\n",
            "Train epoch 16: \tAcc: 66.178 |\n",
            "Test epoch 16: Average losses:\tAcc: 63.320 |\tLoss: 1.004 |\tBpp loss: 0.05 |\tLog loss: 1.02 |\tAux loss: 529.72\n",
            "\n",
            "Time: 63.2211\n",
            "Learning rate: 0.001\n",
            "Train epoch 17: [0/50000 (0%)]\tLoss: 0.933 |\tBpp loss: 0.09 |\tLog loss: 0.93 |\tAux loss: 530.27\n",
            "Train epoch 17: [6400/50000 (13%)]\tLoss: 1.027 |\tBpp loss: 0.09 |\tLog loss: 1.04 |\tAux loss: 499.07\n",
            "Train epoch 17: [12800/50000 (26%)]\tLoss: 0.783 |\tBpp loss: 0.09 |\tLog loss: 0.77 |\tAux loss: 510.92\n",
            "Train epoch 17: [19200/50000 (38%)]\tLoss: 1.098 |\tBpp loss: 0.09 |\tLog loss: 1.12 |\tAux loss: 487.56\n",
            "Train epoch 17: [25600/50000 (51%)]\tLoss: 1.441 |\tBpp loss: 0.09 |\tLog loss: 1.50 |\tAux loss: 491.60\n",
            "Train epoch 17: [32000/50000 (64%)]\tLoss: 1.010 |\tBpp loss: 0.10 |\tLog loss: 1.01 |\tAux loss: 472.60\n",
            "Train epoch 17: [38400/50000 (77%)]\tLoss: 0.816 |\tBpp loss: 0.10 |\tLog loss: 0.80 |\tAux loss: 458.09\n",
            "Train epoch 17: [44800/50000 (90%)]\tLoss: 0.939 |\tBpp loss: 0.09 |\tLog loss: 0.94 |\tAux loss: 471.30\n",
            "\n",
            "Train epoch 17: \tAcc: 67.420 |\n",
            "Test epoch 17: Average losses:\tAcc: 62.360 |\tLoss: 1.049 |\tBpp loss: 0.05 |\tLog loss: 1.01 |\tAux loss: 464.92\n",
            "\n",
            "Time: 63.1153\n",
            "Learning rate: 0.001\n",
            "Train epoch 18: [0/50000 (0%)]\tLoss: 0.847 |\tBpp loss: 0.09 |\tLog loss: 0.84 |\tAux loss: 465.60\n",
            "Train epoch 18: [6400/50000 (13%)]\tLoss: 0.926 |\tBpp loss: 0.09 |\tLog loss: 0.93 |\tAux loss: 455.93\n",
            "Train epoch 18: [12800/50000 (26%)]\tLoss: 0.967 |\tBpp loss: 0.09 |\tLog loss: 0.97 |\tAux loss: 442.59\n",
            "Train epoch 18: [19200/50000 (38%)]\tLoss: 0.763 |\tBpp loss: 0.09 |\tLog loss: 0.75 |\tAux loss: 420.35\n",
            "Train epoch 18: [25600/50000 (51%)]\tLoss: 0.754 |\tBpp loss: 0.09 |\tLog loss: 0.73 |\tAux loss: 417.79\n",
            "Train epoch 18: [32000/50000 (64%)]\tLoss: 1.083 |\tBpp loss: 0.10 |\tLog loss: 1.10 |\tAux loss: 408.97\n",
            "Train epoch 18: [38400/50000 (77%)]\tLoss: 0.979 |\tBpp loss: 0.10 |\tLog loss: 0.98 |\tAux loss: 409.12\n",
            "Train epoch 18: [44800/50000 (90%)]\tLoss: 0.831 |\tBpp loss: 0.09 |\tLog loss: 0.82 |\tAux loss: 381.08\n",
            "\n",
            "Train epoch 18: \tAcc: 69.340 |\n",
            "Test epoch 18: Average losses:\tAcc: 64.440 |\tLoss: 1.014 |\tBpp loss: 0.05 |\tLog loss: 1.08 |\tAux loss: 402.93\n",
            "\n",
            "Time: 63.1601\n",
            "Learning rate: 0.001\n",
            "Train epoch 19: [0/50000 (0%)]\tLoss: 0.740 |\tBpp loss: 0.09 |\tLog loss: 0.72 |\tAux loss: 404.25\n",
            "Train epoch 19: [6400/50000 (13%)]\tLoss: 0.877 |\tBpp loss: 0.09 |\tLog loss: 0.87 |\tAux loss: 391.37\n",
            "Train epoch 19: [12800/50000 (26%)]\tLoss: 1.240 |\tBpp loss: 0.13 |\tLog loss: 1.24 |\tAux loss: 740.61\n",
            "Train epoch 19: [19200/50000 (38%)]\tLoss: 1.112 |\tBpp loss: 0.10 |\tLog loss: 1.13 |\tAux loss: 472.19\n",
            "Train epoch 19: [25600/50000 (51%)]\tLoss: 0.885 |\tBpp loss: 0.10 |\tLog loss: 0.87 |\tAux loss: 399.33\n",
            "Train epoch 19: [32000/50000 (64%)]\tLoss: 0.823 |\tBpp loss: 0.10 |\tLog loss: 0.80 |\tAux loss: 370.34\n",
            "Train epoch 19: [38400/50000 (77%)]\tLoss: 0.731 |\tBpp loss: 0.09 |\tLog loss: 0.71 |\tAux loss: 361.52\n",
            "Train epoch 19: [44800/50000 (90%)]\tLoss: 0.761 |\tBpp loss: 0.10 |\tLog loss: 0.74 |\tAux loss: 352.45\n",
            "\n",
            "Train epoch 19: \tAcc: 69.056 |\n",
            "Test epoch 19: Average losses:\tAcc: 65.280 |\tLoss: 0.936 |\tBpp loss: 0.05 |\tLog loss: 0.95 |\tAux loss: 353.45\n",
            "\n",
            "Time: 63.2282\n",
            "Learning rate: 0.001\n",
            "Train epoch 20: [0/50000 (0%)]\tLoss: 0.577 |\tBpp loss: 0.09 |\tLog loss: 0.54 |\tAux loss: 354.34\n",
            "Train epoch 20: [6400/50000 (13%)]\tLoss: 0.861 |\tBpp loss: 0.09 |\tLog loss: 0.85 |\tAux loss: 349.90\n",
            "Train epoch 20: [12800/50000 (26%)]\tLoss: 0.873 |\tBpp loss: 0.10 |\tLog loss: 0.86 |\tAux loss: 329.12\n",
            "Train epoch 20: [19200/50000 (38%)]\tLoss: 0.842 |\tBpp loss: 0.09 |\tLog loss: 0.83 |\tAux loss: 327.30\n",
            "Train epoch 20: [25600/50000 (51%)]\tLoss: 1.242 |\tBpp loss: 0.10 |\tLog loss: 1.26 |\tAux loss: 743.20\n",
            "Train epoch 20: [32000/50000 (64%)]\tLoss: 0.878 |\tBpp loss: 0.10 |\tLog loss: 0.87 |\tAux loss: 334.74\n",
            "Train epoch 20: [38400/50000 (77%)]\tLoss: 0.673 |\tBpp loss: 0.10 |\tLog loss: 0.64 |\tAux loss: 315.83\n",
            "Train epoch 20: [44800/50000 (90%)]\tLoss: 0.933 |\tBpp loss: 0.10 |\tLog loss: 0.93 |\tAux loss: 305.67\n",
            "\n",
            "Train epoch 20: \tAcc: 68.996 |\n",
            "Test epoch 20: Average losses:\tAcc: 64.820 |\tLoss: 0.967 |\tBpp loss: 0.05 |\tLog loss: 1.07 |\tAux loss: 309.11\n",
            "\n",
            "Time: 63.1980\n",
            "Learning rate: 0.001\n",
            "Train epoch 21: [0/50000 (0%)]\tLoss: 0.804 |\tBpp loss: 0.09 |\tLog loss: 0.79 |\tAux loss: 309.14\n",
            "Train epoch 21: [6400/50000 (13%)]\tLoss: 1.285 |\tBpp loss: 0.12 |\tLog loss: 1.30 |\tAux loss: 838.84\n",
            "Train epoch 21: [12800/50000 (26%)]\tLoss: 0.895 |\tBpp loss: 0.10 |\tLog loss: 0.88 |\tAux loss: 312.41\n",
            "Train epoch 21: [19200/50000 (38%)]\tLoss: 0.868 |\tBpp loss: 0.09 |\tLog loss: 0.86 |\tAux loss: 298.57\n",
            "Train epoch 21: [25600/50000 (51%)]\tLoss: 0.751 |\tBpp loss: 0.10 |\tLog loss: 0.73 |\tAux loss: 289.00\n",
            "Train epoch 21: [32000/50000 (64%)]\tLoss: 0.660 |\tBpp loss: 0.10 |\tLog loss: 0.62 |\tAux loss: 293.93\n",
            "Train epoch 21: [38400/50000 (77%)]\tLoss: 0.713 |\tBpp loss: 0.10 |\tLog loss: 0.68 |\tAux loss: 280.94\n",
            "Train epoch 21: [44800/50000 (90%)]\tLoss: 0.813 |\tBpp loss: 0.10 |\tLog loss: 0.80 |\tAux loss: 281.99\n",
            "\n",
            "Train epoch 21: \tAcc: 69.988 |\n",
            "Test epoch 21: Average losses:\tAcc: 65.680 |\tLoss: 0.922 |\tBpp loss: 0.06 |\tLog loss: 0.96 |\tAux loss: 282.30\n",
            "\n",
            "Time: 63.2116\n",
            "Learning rate: 0.001\n",
            "Train epoch 22: [0/50000 (0%)]\tLoss: 0.715 |\tBpp loss: 0.10 |\tLog loss: 0.69 |\tAux loss: 282.06\n",
            "Train epoch 22: [6400/50000 (13%)]\tLoss: 0.958 |\tBpp loss: 0.10 |\tLog loss: 0.96 |\tAux loss: 272.40\n",
            "Train epoch 22: [12800/50000 (26%)]\tLoss: 0.877 |\tBpp loss: 0.10 |\tLog loss: 0.87 |\tAux loss: 269.55\n",
            "Train epoch 22: [19200/50000 (38%)]\tLoss: 0.724 |\tBpp loss: 0.09 |\tLog loss: 0.70 |\tAux loss: 271.02\n",
            "Train epoch 22: [25600/50000 (51%)]\tLoss: 0.758 |\tBpp loss: 0.10 |\tLog loss: 0.73 |\tAux loss: 272.55\n",
            "Train epoch 22: [32000/50000 (64%)]\tLoss: 0.945 |\tBpp loss: 0.10 |\tLog loss: 0.94 |\tAux loss: 262.42\n",
            "Train epoch 22: [38400/50000 (77%)]\tLoss: 0.675 |\tBpp loss: 0.09 |\tLog loss: 0.64 |\tAux loss: 261.03\n",
            "Train epoch 22: [44800/50000 (90%)]\tLoss: 0.867 |\tBpp loss: 0.10 |\tLog loss: 0.85 |\tAux loss: 258.49\n",
            "\n",
            "Train epoch 22: \tAcc: 73.522 |\n",
            "Test epoch 22: Average losses:\tAcc: 66.320 |\tLoss: 0.910 |\tBpp loss: 0.06 |\tLog loss: 0.95 |\tAux loss: 240.87\n",
            "\n",
            "Time: 63.1903\n",
            "Learning rate: 0.001\n",
            "Train epoch 23: [0/50000 (0%)]\tLoss: 0.670 |\tBpp loss: 0.10 |\tLog loss: 0.63 |\tAux loss: 240.83\n",
            "Train epoch 23: [6400/50000 (13%)]\tLoss: 0.661 |\tBpp loss: 0.10 |\tLog loss: 0.62 |\tAux loss: 229.94\n",
            "Train epoch 23: [12800/50000 (26%)]\tLoss: 0.608 |\tBpp loss: 0.10 |\tLog loss: 0.57 |\tAux loss: 231.83\n",
            "Train epoch 23: [19200/50000 (38%)]\tLoss: 0.692 |\tBpp loss: 0.10 |\tLog loss: 0.66 |\tAux loss: 229.86\n",
            "Train epoch 23: [25600/50000 (51%)]\tLoss: 0.688 |\tBpp loss: 0.10 |\tLog loss: 0.65 |\tAux loss: 235.25\n",
            "Train epoch 23: [32000/50000 (64%)]\tLoss: 0.762 |\tBpp loss: 0.10 |\tLog loss: 0.73 |\tAux loss: 227.68\n",
            "Train epoch 23: [38400/50000 (77%)]\tLoss: 0.970 |\tBpp loss: 0.10 |\tLog loss: 0.97 |\tAux loss: 228.09\n",
            "Train epoch 23: [44800/50000 (90%)]\tLoss: 0.778 |\tBpp loss: 0.10 |\tLog loss: 0.76 |\tAux loss: 244.76\n",
            "\n",
            "Train epoch 23: \tAcc: 75.220 |\n",
            "Test epoch 23: Average losses:\tAcc: 66.970 |\tLoss: 0.924 |\tBpp loss: 0.06 |\tLog loss: 0.93 |\tAux loss: 231.65\n",
            "\n",
            "Time: 62.3781\n",
            "Learning rate: 0.001\n",
            "Train epoch 24: [0/50000 (0%)]\tLoss: 0.578 |\tBpp loss: 0.10 |\tLog loss: 0.53 |\tAux loss: 232.72\n",
            "Train epoch 24: [6400/50000 (13%)]\tLoss: 0.699 |\tBpp loss: 0.10 |\tLog loss: 0.66 |\tAux loss: 222.96\n",
            "Train epoch 24: [12800/50000 (26%)]\tLoss: 0.744 |\tBpp loss: 0.10 |\tLog loss: 0.72 |\tAux loss: 222.03\n",
            "Train epoch 24: [19200/50000 (38%)]\tLoss: 0.693 |\tBpp loss: 0.10 |\tLog loss: 0.66 |\tAux loss: 215.54\n",
            "Train epoch 24: [25600/50000 (51%)]\tLoss: 1.545 |\tBpp loss: 0.14 |\tLog loss: 1.56 |\tAux loss: 1749.05\n",
            "Train epoch 24: [32000/50000 (64%)]\tLoss: 1.164 |\tBpp loss: 0.11 |\tLog loss: 1.17 |\tAux loss: 676.75\n",
            "Train epoch 24: [38400/50000 (77%)]\tLoss: 0.847 |\tBpp loss: 0.11 |\tLog loss: 0.82 |\tAux loss: 249.37\n",
            "Train epoch 24: [44800/50000 (90%)]\tLoss: 0.850 |\tBpp loss: 0.10 |\tLog loss: 0.83 |\tAux loss: 270.68\n",
            "\n",
            "Train epoch 24: \tAcc: 69.640 |\n",
            "Test epoch 24: Average losses:\tAcc: 65.220 |\tLoss: 0.950 |\tBpp loss: 0.05 |\tLog loss: 1.04 |\tAux loss: 264.18\n",
            "\n",
            "Time: 63.1549\n",
            "Learning rate: 0.001\n",
            "Train epoch 25: [0/50000 (0%)]\tLoss: 0.663 |\tBpp loss: 0.10 |\tLog loss: 0.62 |\tAux loss: 263.86\n",
            "Train epoch 25: [6400/50000 (13%)]\tLoss: 0.699 |\tBpp loss: 0.11 |\tLog loss: 0.66 |\tAux loss: 221.11\n",
            "Train epoch 25: [12800/50000 (26%)]\tLoss: 0.615 |\tBpp loss: 0.10 |\tLog loss: 0.57 |\tAux loss: 209.99\n",
            "Train epoch 25: [19200/50000 (38%)]\tLoss: 0.783 |\tBpp loss: 0.10 |\tLog loss: 0.76 |\tAux loss: 196.10\n",
            "Train epoch 25: [25600/50000 (51%)]\tLoss: 0.678 |\tBpp loss: 0.10 |\tLog loss: 0.64 |\tAux loss: 191.01\n",
            "Train epoch 25: [32000/50000 (64%)]\tLoss: 0.692 |\tBpp loss: 0.10 |\tLog loss: 0.66 |\tAux loss: 186.44\n",
            "Train epoch 25: [38400/50000 (77%)]\tLoss: 0.513 |\tBpp loss: 0.10 |\tLog loss: 0.46 |\tAux loss: 182.88\n",
            "Train epoch 25: [44800/50000 (90%)]\tLoss: 0.897 |\tBpp loss: 0.10 |\tLog loss: 0.88 |\tAux loss: 181.73\n",
            "\n",
            "Train epoch 25: \tAcc: 75.802 |\n",
            "Test epoch 25: Average losses:\tAcc: 67.020 |\tLoss: 0.928 |\tBpp loss: 0.06 |\tLog loss: 1.04 |\tAux loss: 178.35\n",
            "\n",
            "Time: 62.8288\n",
            "Learning rate: 0.001\n",
            "Train epoch 26: [0/50000 (0%)]\tLoss: 1.091 |\tBpp loss: 0.10 |\tLog loss: 1.10 |\tAux loss: 178.58\n",
            "Train epoch 26: [6400/50000 (13%)]\tLoss: 0.594 |\tBpp loss: 0.10 |\tLog loss: 0.54 |\tAux loss: 178.03\n",
            "Train epoch 26: [12800/50000 (26%)]\tLoss: 0.650 |\tBpp loss: 0.10 |\tLog loss: 0.61 |\tAux loss: 166.43\n",
            "Train epoch 26: [19200/50000 (38%)]\tLoss: 0.813 |\tBpp loss: 0.10 |\tLog loss: 0.79 |\tAux loss: 172.19\n",
            "Train epoch 26: [25600/50000 (51%)]\tLoss: 0.819 |\tBpp loss: 0.10 |\tLog loss: 0.80 |\tAux loss: 174.98\n",
            "Train epoch 26: [32000/50000 (64%)]\tLoss: 0.596 |\tBpp loss: 0.10 |\tLog loss: 0.55 |\tAux loss: 166.73\n",
            "Train epoch 26: [38400/50000 (77%)]\tLoss: 0.682 |\tBpp loss: 0.10 |\tLog loss: 0.65 |\tAux loss: 160.99\n",
            "Train epoch 26: [44800/50000 (90%)]\tLoss: 0.629 |\tBpp loss: 0.11 |\tLog loss: 0.58 |\tAux loss: 166.10\n",
            "\n",
            "Train epoch 26: \tAcc: 78.434 |\n",
            "Test epoch 26: Average losses:\tAcc: 67.240 |\tLoss: 1.084 |\tBpp loss: 0.06 |\tLog loss: 0.91 |\tAux loss: 163.83\n",
            "\n",
            "Time: 63.2837\n",
            "Learning rate: 0.001\n",
            "Train epoch 27: [0/50000 (0%)]\tLoss: 0.578 |\tBpp loss: 0.10 |\tLog loss: 0.53 |\tAux loss: 164.85\n",
            "Train epoch 27: [6400/50000 (13%)]\tLoss: 0.629 |\tBpp loss: 0.10 |\tLog loss: 0.59 |\tAux loss: 157.44\n",
            "Train epoch 27: [12800/50000 (26%)]\tLoss: 0.750 |\tBpp loss: 0.11 |\tLog loss: 0.72 |\tAux loss: 160.72\n",
            "Train epoch 27: [19200/50000 (38%)]\tLoss: 0.726 |\tBpp loss: 0.10 |\tLog loss: 0.70 |\tAux loss: 150.92\n",
            "Train epoch 27: [25600/50000 (51%)]\tLoss: 0.555 |\tBpp loss: 0.10 |\tLog loss: 0.50 |\tAux loss: 151.37\n",
            "Train epoch 27: [32000/50000 (64%)]\tLoss: 0.650 |\tBpp loss: 0.10 |\tLog loss: 0.61 |\tAux loss: 147.82\n",
            "Train epoch 27: [38400/50000 (77%)]\tLoss: 0.523 |\tBpp loss: 0.10 |\tLog loss: 0.47 |\tAux loss: 143.16\n",
            "Train epoch 27: [44800/50000 (90%)]\tLoss: 0.535 |\tBpp loss: 0.10 |\tLog loss: 0.48 |\tAux loss: 144.69\n",
            "\n",
            "Train epoch 27: \tAcc: 79.382 |\n",
            "Test epoch 27: Average losses:\tAcc: 65.970 |\tLoss: 1.000 |\tBpp loss: 0.06 |\tLog loss: 0.95 |\tAux loss: 139.96\n",
            "\n",
            "Time: 62.8526\n",
            "Learning rate: 0.001\n",
            "Train epoch 28: [0/50000 (0%)]\tLoss: 0.602 |\tBpp loss: 0.10 |\tLog loss: 0.56 |\tAux loss: 140.38\n",
            "Train epoch 28: [6400/50000 (13%)]\tLoss: 0.752 |\tBpp loss: 0.10 |\tLog loss: 0.72 |\tAux loss: 136.94\n",
            "Train epoch 28: [12800/50000 (26%)]\tLoss: 0.631 |\tBpp loss: 0.10 |\tLog loss: 0.59 |\tAux loss: 135.98\n",
            "Train epoch 28: [19200/50000 (38%)]\tLoss: 0.623 |\tBpp loss: 0.10 |\tLog loss: 0.58 |\tAux loss: 138.41\n",
            "Train epoch 28: [25600/50000 (51%)]\tLoss: 0.803 |\tBpp loss: 0.11 |\tLog loss: 0.78 |\tAux loss: 136.47\n",
            "Train epoch 28: [32000/50000 (64%)]\tLoss: 0.519 |\tBpp loss: 0.11 |\tLog loss: 0.46 |\tAux loss: 129.62\n",
            "Train epoch 28: [38400/50000 (77%)]\tLoss: 0.625 |\tBpp loss: 0.10 |\tLog loss: 0.58 |\tAux loss: 136.44\n",
            "Train epoch 28: [44800/50000 (90%)]\tLoss: 0.631 |\tBpp loss: 0.10 |\tLog loss: 0.59 |\tAux loss: 132.99\n",
            "\n",
            "Train epoch 28: \tAcc: 80.268 |\n",
            "Test epoch 28: Average losses:\tAcc: 67.390 |\tLoss: 0.952 |\tBpp loss: 0.06 |\tLog loss: 0.95 |\tAux loss: 134.89\n",
            "\n",
            "Time: 63.5723\n",
            "Learning rate: 0.001\n",
            "Train epoch 29: [0/50000 (0%)]\tLoss: 0.526 |\tBpp loss: 0.10 |\tLog loss: 0.47 |\tAux loss: 135.69\n",
            "Train epoch 29: [6400/50000 (13%)]\tLoss: 0.520 |\tBpp loss: 0.10 |\tLog loss: 0.46 |\tAux loss: 125.33\n",
            "Train epoch 29: [12800/50000 (26%)]\tLoss: 0.550 |\tBpp loss: 0.11 |\tLog loss: 0.49 |\tAux loss: 123.94\n",
            "Train epoch 29: [19200/50000 (38%)]\tLoss: 0.451 |\tBpp loss: 0.11 |\tLog loss: 0.38 |\tAux loss: 119.61\n",
            "Train epoch 29: [25600/50000 (51%)]\tLoss: 0.538 |\tBpp loss: 0.10 |\tLog loss: 0.48 |\tAux loss: 124.28\n",
            "Train epoch 29: [32000/50000 (64%)]\tLoss: 0.835 |\tBpp loss: 0.12 |\tLog loss: 0.80 |\tAux loss: 125.82\n",
            "Train epoch 29: [38400/50000 (77%)]\tLoss: 0.686 |\tBpp loss: 0.11 |\tLog loss: 0.64 |\tAux loss: 119.91\n",
            "Train epoch 29: [44800/50000 (90%)]\tLoss: 0.697 |\tBpp loss: 0.11 |\tLog loss: 0.65 |\tAux loss: 111.25\n",
            "\n",
            "Train epoch 29: \tAcc: 81.394 |\n",
            "Test epoch 29: Average losses:\tAcc: 68.830 |\tLoss: 1.008 |\tBpp loss: 0.06 |\tLog loss: 0.98 |\tAux loss: 113.22\n",
            "\n",
            "Time: 62.6221\n",
            "Learning rate: 0.001\n",
            "Train epoch 30: [0/50000 (0%)]\tLoss: 0.347 |\tBpp loss: 0.11 |\tLog loss: 0.27 |\tAux loss: 113.52\n",
            "Train epoch 30: [6400/50000 (13%)]\tLoss: 0.502 |\tBpp loss: 0.10 |\tLog loss: 0.44 |\tAux loss: 103.49\n",
            "Train epoch 30: [12800/50000 (26%)]\tLoss: 0.602 |\tBpp loss: 0.10 |\tLog loss: 0.55 |\tAux loss: 107.93\n",
            "Train epoch 30: [19200/50000 (38%)]\tLoss: 0.495 |\tBpp loss: 0.10 |\tLog loss: 0.44 |\tAux loss: 100.61\n",
            "Train epoch 30: [25600/50000 (51%)]\tLoss: 0.599 |\tBpp loss: 0.10 |\tLog loss: 0.55 |\tAux loss: 104.54\n",
            "Train epoch 30: [32000/50000 (64%)]\tLoss: 0.578 |\tBpp loss: 0.10 |\tLog loss: 0.53 |\tAux loss: 100.64\n",
            "Train epoch 30: [38400/50000 (77%)]\tLoss: 0.490 |\tBpp loss: 0.10 |\tLog loss: 0.43 |\tAux loss: 101.77\n",
            "Train epoch 30: [44800/50000 (90%)]\tLoss: 0.324 |\tBpp loss: 0.10 |\tLog loss: 0.24 |\tAux loss: 100.52\n",
            "\n",
            "Train epoch 30: \tAcc: 82.490 |\n",
            "Test epoch 30: Average losses:\tAcc: 67.170 |\tLoss: 1.282 |\tBpp loss: 0.06 |\tLog loss: 1.06 |\tAux loss: 118.14\n",
            "\n",
            "Time: 62.6930\n",
            "Learning rate: 0.001\n",
            "Train epoch 31: [0/50000 (0%)]\tLoss: 0.436 |\tBpp loss: 0.11 |\tLog loss: 0.37 |\tAux loss: 119.41\n",
            "Train epoch 31: [6400/50000 (13%)]\tLoss: 0.460 |\tBpp loss: 0.10 |\tLog loss: 0.40 |\tAux loss: 100.98\n",
            "Train epoch 31: [12800/50000 (26%)]\tLoss: 0.523 |\tBpp loss: 0.10 |\tLog loss: 0.47 |\tAux loss: 95.64\n",
            "Train epoch 31: [19200/50000 (38%)]\tLoss: 0.485 |\tBpp loss: 0.11 |\tLog loss: 0.42 |\tAux loss: 92.49\n",
            "Train epoch 31: [25600/50000 (51%)]\tLoss: 0.446 |\tBpp loss: 0.11 |\tLog loss: 0.38 |\tAux loss: 101.19\n",
            "Train epoch 31: [32000/50000 (64%)]\tLoss: 0.685 |\tBpp loss: 0.10 |\tLog loss: 0.65 |\tAux loss: 89.77\n",
            "Train epoch 31: [38400/50000 (77%)]\tLoss: 0.834 |\tBpp loss: 0.12 |\tLog loss: 0.80 |\tAux loss: 269.71\n",
            "Train epoch 31: [44800/50000 (90%)]\tLoss: 0.522 |\tBpp loss: 0.11 |\tLog loss: 0.46 |\tAux loss: 92.08\n",
            "\n",
            "Train epoch 31: \tAcc: 82.112 |\n",
            "Test epoch 31: Average losses:\tAcc: 64.350 |\tLoss: 1.633 |\tBpp loss: 0.06 |\tLog loss: 2.51 |\tAux loss: 93.14\n",
            "\n",
            "Time: 63.1430\n",
            "Learning rate: 0.001\n",
            "Train epoch 32: [0/50000 (0%)]\tLoss: 0.731 |\tBpp loss: 0.11 |\tLog loss: 0.69 |\tAux loss: 95.10\n",
            "Train epoch 32: [6400/50000 (13%)]\tLoss: 0.951 |\tBpp loss: 0.10 |\tLog loss: 0.94 |\tAux loss: 88.91\n",
            "Train epoch 32: [12800/50000 (26%)]\tLoss: 0.509 |\tBpp loss: 0.11 |\tLog loss: 0.45 |\tAux loss: 81.31\n",
            "Train epoch 32: [19200/50000 (38%)]\tLoss: 0.504 |\tBpp loss: 0.11 |\tLog loss: 0.44 |\tAux loss: 77.84\n",
            "Train epoch 32: [25600/50000 (51%)]\tLoss: 0.575 |\tBpp loss: 0.11 |\tLog loss: 0.51 |\tAux loss: 147.44\n",
            "Train epoch 32: [32000/50000 (64%)]\tLoss: 0.574 |\tBpp loss: 0.11 |\tLog loss: 0.51 |\tAux loss: 91.75\n",
            "Train epoch 32: [38400/50000 (77%)]\tLoss: 0.592 |\tBpp loss: 0.12 |\tLog loss: 0.52 |\tAux loss: 153.15\n",
            "Train epoch 32: [44800/50000 (90%)]\tLoss: 0.614 |\tBpp loss: 0.11 |\tLog loss: 0.56 |\tAux loss: 70.35\n",
            "\n",
            "Train epoch 32: \tAcc: 82.812 |\n",
            "Test epoch 32: Average losses:\tAcc: 67.400 |\tLoss: 1.020 |\tBpp loss: 0.06 |\tLog loss: 1.22 |\tAux loss: 74.08\n",
            "\n",
            "Time: 63.2497\n",
            "Learning rate: 0.001\n",
            "Train epoch 33: [0/50000 (0%)]\tLoss: 0.493 |\tBpp loss: 0.11 |\tLog loss: 0.43 |\tAux loss: 74.76\n",
            "Train epoch 33: [6400/50000 (13%)]\tLoss: 0.403 |\tBpp loss: 0.11 |\tLog loss: 0.33 |\tAux loss: 64.28\n",
            "Train epoch 33: [12800/50000 (26%)]\tLoss: 0.509 |\tBpp loss: 0.11 |\tLog loss: 0.45 |\tAux loss: 82.30\n",
            "Train epoch 33: [19200/50000 (38%)]\tLoss: 0.528 |\tBpp loss: 0.11 |\tLog loss: 0.46 |\tAux loss: 71.20\n",
            "Train epoch 33: [25600/50000 (51%)]\tLoss: 0.479 |\tBpp loss: 0.11 |\tLog loss: 0.41 |\tAux loss: 65.36\n",
            "Train epoch 33: [32000/50000 (64%)]\tLoss: 0.403 |\tBpp loss: 0.11 |\tLog loss: 0.33 |\tAux loss: 59.69\n",
            "Train epoch 33: [38400/50000 (77%)]\tLoss: 0.604 |\tBpp loss: 0.11 |\tLog loss: 0.55 |\tAux loss: 66.62\n",
            "Train epoch 33: [44800/50000 (90%)]\tLoss: 0.379 |\tBpp loss: 0.10 |\tLog loss: 0.31 |\tAux loss: 57.68\n",
            "\n",
            "Train epoch 33: \tAcc: 85.126 |\n",
            "Test epoch 33: Average losses:\tAcc: 68.190 |\tLoss: 1.024 |\tBpp loss: 0.07 |\tLog loss: 1.13 |\tAux loss: 58.89\n",
            "\n",
            "Time: 63.0885\n",
            "Learning rate: 0.0005\n",
            "Train epoch 34: [0/50000 (0%)]\tLoss: 0.308 |\tBpp loss: 0.11 |\tLog loss: 0.22 |\tAux loss: 59.00\n",
            "Train epoch 34: [6400/50000 (13%)]\tLoss: 0.390 |\tBpp loss: 0.10 |\tLog loss: 0.32 |\tAux loss: 49.08\n",
            "Train epoch 34: [12800/50000 (26%)]\tLoss: 0.468 |\tBpp loss: 0.10 |\tLog loss: 0.41 |\tAux loss: 48.21\n",
            "Train epoch 34: [19200/50000 (38%)]\tLoss: 0.423 |\tBpp loss: 0.10 |\tLog loss: 0.36 |\tAux loss: 47.24\n",
            "Train epoch 34: [25600/50000 (51%)]\tLoss: 0.512 |\tBpp loss: 0.10 |\tLog loss: 0.46 |\tAux loss: 48.34\n",
            "Train epoch 34: [32000/50000 (64%)]\tLoss: 0.262 |\tBpp loss: 0.10 |\tLog loss: 0.18 |\tAux loss: 45.49\n",
            "Train epoch 34: [38400/50000 (77%)]\tLoss: 0.382 |\tBpp loss: 0.10 |\tLog loss: 0.31 |\tAux loss: 44.10\n",
            "Train epoch 34: [44800/50000 (90%)]\tLoss: 0.305 |\tBpp loss: 0.10 |\tLog loss: 0.22 |\tAux loss: 42.60\n",
            "\n",
            "Train epoch 34: \tAcc: 89.228 |\n",
            "Test epoch 34: Average losses:\tAcc: 68.870 |\tLoss: 1.085 |\tBpp loss: 0.06 |\tLog loss: 1.38 |\tAux loss: 42.19\n",
            "\n",
            "Time: 63.0211\n",
            "Learning rate: 0.0005\n",
            "Train epoch 35: [0/50000 (0%)]\tLoss: 0.274 |\tBpp loss: 0.10 |\tLog loss: 0.19 |\tAux loss: 42.16\n",
            "Train epoch 35: [6400/50000 (13%)]\tLoss: 0.238 |\tBpp loss: 0.10 |\tLog loss: 0.15 |\tAux loss: 40.09\n",
            "Train epoch 35: [12800/50000 (26%)]\tLoss: 0.340 |\tBpp loss: 0.10 |\tLog loss: 0.26 |\tAux loss: 41.60\n",
            "Train epoch 35: [19200/50000 (38%)]\tLoss: 0.356 |\tBpp loss: 0.10 |\tLog loss: 0.28 |\tAux loss: 39.59\n",
            "Train epoch 35: [25600/50000 (51%)]\tLoss: 0.286 |\tBpp loss: 0.10 |\tLog loss: 0.21 |\tAux loss: 37.26\n",
            "Train epoch 35: [32000/50000 (64%)]\tLoss: 0.260 |\tBpp loss: 0.10 |\tLog loss: 0.18 |\tAux loss: 41.33\n",
            "Train epoch 35: [38400/50000 (77%)]\tLoss: 0.397 |\tBpp loss: 0.10 |\tLog loss: 0.33 |\tAux loss: 35.66\n",
            "Train epoch 35: [44800/50000 (90%)]\tLoss: 0.341 |\tBpp loss: 0.10 |\tLog loss: 0.27 |\tAux loss: 34.39\n",
            "\n",
            "Train epoch 35: \tAcc: 90.400 |\n",
            "Test epoch 35: Average losses:\tAcc: 68.560 |\tLoss: 1.102 |\tBpp loss: 0.07 |\tLog loss: 1.27 |\tAux loss: 33.52\n",
            "\n",
            "Time: 62.9512\n",
            "Learning rate: 0.0005\n",
            "Train epoch 36: [0/50000 (0%)]\tLoss: 0.377 |\tBpp loss: 0.10 |\tLog loss: 0.31 |\tAux loss: 33.89\n",
            "Train epoch 36: [6400/50000 (13%)]\tLoss: 0.340 |\tBpp loss: 0.11 |\tLog loss: 0.26 |\tAux loss: 30.73\n",
            "Train epoch 36: [12800/50000 (26%)]\tLoss: 0.466 |\tBpp loss: 0.10 |\tLog loss: 0.41 |\tAux loss: 29.94\n",
            "Train epoch 36: [19200/50000 (38%)]\tLoss: 0.310 |\tBpp loss: 0.11 |\tLog loss: 0.23 |\tAux loss: 28.74\n",
            "Train epoch 36: [25600/50000 (51%)]\tLoss: 0.308 |\tBpp loss: 0.10 |\tLog loss: 0.23 |\tAux loss: 26.76\n",
            "Train epoch 36: [32000/50000 (64%)]\tLoss: 0.294 |\tBpp loss: 0.10 |\tLog loss: 0.21 |\tAux loss: 26.46\n",
            "Train epoch 36: [38400/50000 (77%)]\tLoss: 0.306 |\tBpp loss: 0.10 |\tLog loss: 0.23 |\tAux loss: 27.68\n",
            "Train epoch 36: [44800/50000 (90%)]\tLoss: 0.394 |\tBpp loss: 0.10 |\tLog loss: 0.32 |\tAux loss: 23.72\n",
            "\n",
            "Train epoch 36: \tAcc: 90.978 |\n",
            "Test epoch 36: Average losses:\tAcc: 68.380 |\tLoss: 1.150 |\tBpp loss: 0.07 |\tLog loss: 1.25 |\tAux loss: 23.49\n",
            "\n",
            "Time: 62.7606\n",
            "Learning rate: 0.0005\n",
            "Train epoch 37: [0/50000 (0%)]\tLoss: 0.227 |\tBpp loss: 0.10 |\tLog loss: 0.14 |\tAux loss: 23.76\n",
            "Train epoch 37: [6400/50000 (13%)]\tLoss: 0.271 |\tBpp loss: 0.10 |\tLog loss: 0.19 |\tAux loss: 21.83\n",
            "Train epoch 37: [12800/50000 (26%)]\tLoss: 0.329 |\tBpp loss: 0.10 |\tLog loss: 0.25 |\tAux loss: 20.40\n",
            "Train epoch 37: [19200/50000 (38%)]\tLoss: 0.393 |\tBpp loss: 0.11 |\tLog loss: 0.32 |\tAux loss: 20.92\n",
            "Train epoch 37: [25600/50000 (51%)]\tLoss: 0.336 |\tBpp loss: 0.11 |\tLog loss: 0.26 |\tAux loss: 26.38\n",
            "Train epoch 37: [32000/50000 (64%)]\tLoss: 0.299 |\tBpp loss: 0.10 |\tLog loss: 0.22 |\tAux loss: 19.69\n",
            "Train epoch 37: [38400/50000 (77%)]\tLoss: 0.239 |\tBpp loss: 0.10 |\tLog loss: 0.15 |\tAux loss: 23.44\n",
            "Train epoch 37: [44800/50000 (90%)]\tLoss: 0.315 |\tBpp loss: 0.10 |\tLog loss: 0.24 |\tAux loss: 22.32\n",
            "\n",
            "Train epoch 37: \tAcc: 91.626 |\n",
            "Test epoch 37: Average losses:\tAcc: 67.860 |\tLoss: 1.213 |\tBpp loss: 0.07 |\tLog loss: 1.51 |\tAux loss: 23.78\n",
            "\n",
            "Time: 62.4550\n",
            "Learning rate: 0.0005\n",
            "Train epoch 38: [0/50000 (0%)]\tLoss: 0.288 |\tBpp loss: 0.10 |\tLog loss: 0.21 |\tAux loss: 23.98\n",
            "Train epoch 38: [6400/50000 (13%)]\tLoss: 0.388 |\tBpp loss: 0.10 |\tLog loss: 0.32 |\tAux loss: 22.06\n",
            "Train epoch 38: [12800/50000 (26%)]\tLoss: 0.339 |\tBpp loss: 0.11 |\tLog loss: 0.26 |\tAux loss: 54.41\n",
            "Train epoch 38: [19200/50000 (38%)]\tLoss: 0.218 |\tBpp loss: 0.10 |\tLog loss: 0.13 |\tAux loss: 21.35\n",
            "Train epoch 38: [25600/50000 (51%)]\tLoss: 0.307 |\tBpp loss: 0.11 |\tLog loss: 0.22 |\tAux loss: 17.19\n",
            "Train epoch 38: [32000/50000 (64%)]\tLoss: 0.347 |\tBpp loss: 0.10 |\tLog loss: 0.27 |\tAux loss: 16.17\n",
            "Train epoch 38: [38400/50000 (77%)]\tLoss: 0.407 |\tBpp loss: 0.11 |\tLog loss: 0.33 |\tAux loss: 17.73\n",
            "Train epoch 38: [44800/50000 (90%)]\tLoss: 0.339 |\tBpp loss: 0.10 |\tLog loss: 0.26 |\tAux loss: 19.08\n",
            "\n",
            "Train epoch 38: \tAcc: 90.552 |\n",
            "Test epoch 38: Average losses:\tAcc: 68.650 |\tLoss: 1.168 |\tBpp loss: 0.07 |\tLog loss: 1.51 |\tAux loss: 18.68\n",
            "\n",
            "Time: 63.4205\n",
            "Learning rate: 0.0005\n",
            "Train epoch 39: [0/50000 (0%)]\tLoss: 0.226 |\tBpp loss: 0.10 |\tLog loss: 0.14 |\tAux loss: 18.74\n",
            "Train epoch 39: [6400/50000 (13%)]\tLoss: 0.346 |\tBpp loss: 0.10 |\tLog loss: 0.27 |\tAux loss: 17.08\n",
            "Train epoch 39: [12800/50000 (26%)]\tLoss: 0.322 |\tBpp loss: 0.10 |\tLog loss: 0.25 |\tAux loss: 17.38\n",
            "Train epoch 39: [19200/50000 (38%)]\tLoss: 0.352 |\tBpp loss: 0.10 |\tLog loss: 0.28 |\tAux loss: 20.10\n",
            "Train epoch 39: [25600/50000 (51%)]\tLoss: 0.336 |\tBpp loss: 0.10 |\tLog loss: 0.26 |\tAux loss: 19.85\n",
            "Train epoch 39: [32000/50000 (64%)]\tLoss: 0.236 |\tBpp loss: 0.10 |\tLog loss: 0.15 |\tAux loss: 22.06\n",
            "Train epoch 39: [38400/50000 (77%)]\tLoss: 0.236 |\tBpp loss: 0.10 |\tLog loss: 0.15 |\tAux loss: 22.46\n",
            "Train epoch 39: [44800/50000 (90%)]\tLoss: 0.248 |\tBpp loss: 0.10 |\tLog loss: 0.16 |\tAux loss: 20.30\n",
            "\n",
            "Train epoch 39: \tAcc: 93.136 |\n",
            "Test epoch 39: Average losses:\tAcc: 67.880 |\tLoss: 1.268 |\tBpp loss: 0.07 |\tLog loss: 1.54 |\tAux loss: 21.59\n",
            "\n",
            "Time: 62.9997\n",
            "Learning rate: 0.0005\n",
            "Train epoch 40: [0/50000 (0%)]\tLoss: 0.339 |\tBpp loss: 0.10 |\tLog loss: 0.26 |\tAux loss: 21.76\n",
            "Train epoch 40: [6400/50000 (13%)]\tLoss: 0.402 |\tBpp loss: 0.10 |\tLog loss: 0.33 |\tAux loss: 21.01\n",
            "Train epoch 40: [12800/50000 (26%)]\tLoss: 0.432 |\tBpp loss: 0.10 |\tLog loss: 0.36 |\tAux loss: 22.32\n",
            "Train epoch 40: [19200/50000 (38%)]\tLoss: 0.189 |\tBpp loss: 0.10 |\tLog loss: 0.10 |\tAux loss: 20.81\n",
            "Train epoch 40: [25600/50000 (51%)]\tLoss: 0.301 |\tBpp loss: 0.11 |\tLog loss: 0.22 |\tAux loss: 22.39\n",
            "Train epoch 40: [32000/50000 (64%)]\tLoss: 0.282 |\tBpp loss: 0.10 |\tLog loss: 0.20 |\tAux loss: 22.78\n",
            "Train epoch 40: [38400/50000 (77%)]\tLoss: 0.263 |\tBpp loss: 0.10 |\tLog loss: 0.18 |\tAux loss: 22.42\n",
            "Train epoch 40: [44800/50000 (90%)]\tLoss: 0.196 |\tBpp loss: 0.10 |\tLog loss: 0.11 |\tAux loss: 21.23\n",
            "\n",
            "Train epoch 40: \tAcc: 93.188 |\n",
            "Test epoch 40: Average losses:\tAcc: 65.000 |\tLoss: 1.323 |\tBpp loss: 0.07 |\tLog loss: 1.93 |\tAux loss: 38.62\n",
            "\n",
            "Time: 62.6508\n",
            "Learning rate: 0.0005\n",
            "Train epoch 41: [0/50000 (0%)]\tLoss: 0.565 |\tBpp loss: 0.11 |\tLog loss: 0.51 |\tAux loss: 36.52\n",
            "Train epoch 41: [6400/50000 (13%)]\tLoss: 0.279 |\tBpp loss: 0.11 |\tLog loss: 0.19 |\tAux loss: 22.52\n",
            "Train epoch 41: [12800/50000 (26%)]\tLoss: 0.343 |\tBpp loss: 0.10 |\tLog loss: 0.27 |\tAux loss: 17.29\n",
            "Train epoch 41: [19200/50000 (38%)]\tLoss: 0.234 |\tBpp loss: 0.10 |\tLog loss: 0.15 |\tAux loss: 17.97\n",
            "Train epoch 41: [25600/50000 (51%)]\tLoss: 0.271 |\tBpp loss: 0.10 |\tLog loss: 0.19 |\tAux loss: 17.88\n",
            "Train epoch 41: [32000/50000 (64%)]\tLoss: 0.362 |\tBpp loss: 0.10 |\tLog loss: 0.29 |\tAux loss: 17.03\n",
            "Train epoch 41: [38400/50000 (77%)]\tLoss: 0.213 |\tBpp loss: 0.10 |\tLog loss: 0.12 |\tAux loss: 21.43\n",
            "Train epoch 41: [44800/50000 (90%)]\tLoss: 0.199 |\tBpp loss: 0.11 |\tLog loss: 0.10 |\tAux loss: 22.17\n",
            "\n",
            "Train epoch 41: \tAcc: 93.032 |\n",
            "Test epoch 41: Average losses:\tAcc: 67.830 |\tLoss: 1.318 |\tBpp loss: 0.07 |\tLog loss: 1.74 |\tAux loss: 22.02\n",
            "\n",
            "Time: 62.4253\n",
            "Learning rate: 0.0005\n",
            "Train epoch 42: [0/50000 (0%)]\tLoss: 0.315 |\tBpp loss: 0.10 |\tLog loss: 0.24 |\tAux loss: 22.31\n",
            "Train epoch 42: [6400/50000 (13%)]\tLoss: 0.372 |\tBpp loss: 0.11 |\tLog loss: 0.30 |\tAux loss: 19.68\n",
            "Train epoch 42: [12800/50000 (26%)]\tLoss: 0.163 |\tBpp loss: 0.10 |\tLog loss: 0.07 |\tAux loss: 21.13\n",
            "Train epoch 42: [19200/50000 (38%)]\tLoss: 0.353 |\tBpp loss: 0.10 |\tLog loss: 0.28 |\tAux loss: 19.92\n",
            "Train epoch 42: [25600/50000 (51%)]\tLoss: 0.268 |\tBpp loss: 0.10 |\tLog loss: 0.19 |\tAux loss: 20.01\n",
            "Train epoch 42: [32000/50000 (64%)]\tLoss: 0.202 |\tBpp loss: 0.10 |\tLog loss: 0.11 |\tAux loss: 22.51\n",
            "Train epoch 42: [38400/50000 (77%)]\tLoss: 0.263 |\tBpp loss: 0.10 |\tLog loss: 0.18 |\tAux loss: 20.22\n",
            "Train epoch 42: [44800/50000 (90%)]\tLoss: 0.262 |\tBpp loss: 0.10 |\tLog loss: 0.18 |\tAux loss: 22.88\n",
            "\n",
            "Train epoch 42: \tAcc: 94.076 |\n",
            "Test epoch 42: Average losses:\tAcc: 67.930 |\tLoss: 1.375 |\tBpp loss: 0.07 |\tLog loss: 1.69 |\tAux loss: 21.78\n",
            "\n",
            "Time: 63.0618\n",
            "Learning rate: 0.0005\n",
            "Train epoch 43: [0/50000 (0%)]\tLoss: 0.218 |\tBpp loss: 0.10 |\tLog loss: 0.13 |\tAux loss: 22.10\n",
            "Train epoch 43: [6400/50000 (13%)]\tLoss: 0.235 |\tBpp loss: 0.10 |\tLog loss: 0.15 |\tAux loss: 19.48\n",
            "Train epoch 43: [12800/50000 (26%)]\tLoss: 0.163 |\tBpp loss: 0.10 |\tLog loss: 0.07 |\tAux loss: 21.57\n",
            "Train epoch 43: [19200/50000 (38%)]\tLoss: 0.470 |\tBpp loss: 0.10 |\tLog loss: 0.41 |\tAux loss: 23.20\n",
            "Train epoch 43: [25600/50000 (51%)]\tLoss: 0.215 |\tBpp loss: 0.10 |\tLog loss: 0.12 |\tAux loss: 22.44\n",
            "Train epoch 43: [32000/50000 (64%)]\tLoss: 0.206 |\tBpp loss: 0.10 |\tLog loss: 0.12 |\tAux loss: 19.69\n",
            "Train epoch 43: [38400/50000 (77%)]\tLoss: 0.322 |\tBpp loss: 0.10 |\tLog loss: 0.24 |\tAux loss: 20.34\n",
            "Train epoch 43: [44800/50000 (90%)]\tLoss: 0.282 |\tBpp loss: 0.11 |\tLog loss: 0.20 |\tAux loss: 21.74\n",
            "\n",
            "Train epoch 43: \tAcc: 94.646 |\n",
            "Test epoch 43: Average losses:\tAcc: 67.730 |\tLoss: 1.388 |\tBpp loss: 0.07 |\tLog loss: 1.69 |\tAux loss: 20.19\n",
            "\n",
            "Time: 62.7781\n",
            "Learning rate: 0.0005\n",
            "Train epoch 44: [0/50000 (0%)]\tLoss: 0.233 |\tBpp loss: 0.10 |\tLog loss: 0.15 |\tAux loss: 20.39\n",
            "Train epoch 44: [6400/50000 (13%)]\tLoss: 0.238 |\tBpp loss: 0.10 |\tLog loss: 0.15 |\tAux loss: 21.30\n",
            "Train epoch 44: [12800/50000 (26%)]\tLoss: 0.368 |\tBpp loss: 0.10 |\tLog loss: 0.30 |\tAux loss: 20.04\n",
            "Train epoch 44: [19200/50000 (38%)]\tLoss: 0.238 |\tBpp loss: 0.10 |\tLog loss: 0.15 |\tAux loss: 23.35\n",
            "Train epoch 44: [25600/50000 (51%)]\tLoss: 0.235 |\tBpp loss: 0.10 |\tLog loss: 0.15 |\tAux loss: 21.23\n",
            "Train epoch 44: [32000/50000 (64%)]\tLoss: 0.197 |\tBpp loss: 0.10 |\tLog loss: 0.11 |\tAux loss: 20.94\n",
            "Train epoch 44: [38400/50000 (77%)]\tLoss: 0.288 |\tBpp loss: 0.10 |\tLog loss: 0.20 |\tAux loss: 19.92\n",
            "Train epoch 44: [44800/50000 (90%)]\tLoss: 0.279 |\tBpp loss: 0.10 |\tLog loss: 0.19 |\tAux loss: 21.05\n",
            "\n",
            "Train epoch 44: \tAcc: 94.914 |\n",
            "Test epoch 44: Average losses:\tAcc: 67.210 |\tLoss: 1.525 |\tBpp loss: 0.07 |\tLog loss: 1.73 |\tAux loss: 19.77\n",
            "\n",
            "Time: 62.9882\n",
            "Learning rate: 0.00025\n",
            "Train epoch 45: [0/50000 (0%)]\tLoss: 0.295 |\tBpp loss: 0.10 |\tLog loss: 0.22 |\tAux loss: 19.79\n",
            "Train epoch 45: [6400/50000 (13%)]\tLoss: 0.171 |\tBpp loss: 0.10 |\tLog loss: 0.08 |\tAux loss: 18.22\n",
            "Train epoch 45: [12800/50000 (26%)]\tLoss: 0.201 |\tBpp loss: 0.11 |\tLog loss: 0.10 |\tAux loss: 18.72\n",
            "Train epoch 45: [19200/50000 (38%)]\tLoss: 0.126 |\tBpp loss: 0.10 |\tLog loss: 0.03 |\tAux loss: 20.43\n",
            "Train epoch 45: [25600/50000 (51%)]\tLoss: 0.293 |\tBpp loss: 0.10 |\tLog loss: 0.22 |\tAux loss: 18.37\n",
            "Train epoch 45: [32000/50000 (64%)]\tLoss: 0.222 |\tBpp loss: 0.10 |\tLog loss: 0.14 |\tAux loss: 19.71\n",
            "Train epoch 45: [38400/50000 (77%)]\tLoss: 0.312 |\tBpp loss: 0.10 |\tLog loss: 0.23 |\tAux loss: 19.40\n",
            "Train epoch 45: [44800/50000 (90%)]\tLoss: 0.302 |\tBpp loss: 0.10 |\tLog loss: 0.23 |\tAux loss: 20.03\n",
            "\n",
            "Train epoch 45: \tAcc: 96.166 |\n",
            "Test epoch 45: Average losses:\tAcc: 68.280 |\tLoss: 1.488 |\tBpp loss: 0.07 |\tLog loss: 1.91 |\tAux loss: 20.67\n",
            "\n",
            "Time: 63.2279\n",
            "Learning rate: 0.00025\n",
            "Train epoch 46: [0/50000 (0%)]\tLoss: 0.119 |\tBpp loss: 0.10 |\tLog loss: 0.02 |\tAux loss: 20.80\n",
            "Train epoch 46: [6400/50000 (13%)]\tLoss: 0.202 |\tBpp loss: 0.10 |\tLog loss: 0.11 |\tAux loss: 22.31\n",
            "Train epoch 46: [12800/50000 (26%)]\tLoss: 0.127 |\tBpp loss: 0.10 |\tLog loss: 0.03 |\tAux loss: 22.10\n",
            "Train epoch 46: [19200/50000 (38%)]\tLoss: 0.207 |\tBpp loss: 0.10 |\tLog loss: 0.12 |\tAux loss: 23.27\n",
            "Train epoch 46: [25600/50000 (51%)]\tLoss: 0.128 |\tBpp loss: 0.10 |\tLog loss: 0.03 |\tAux loss: 21.94\n",
            "Train epoch 46: [32000/50000 (64%)]\tLoss: 0.214 |\tBpp loss: 0.09 |\tLog loss: 0.13 |\tAux loss: 22.05\n",
            "Train epoch 46: [38400/50000 (77%)]\tLoss: 0.137 |\tBpp loss: 0.10 |\tLog loss: 0.04 |\tAux loss: 23.66\n",
            "Train epoch 46: [44800/50000 (90%)]\tLoss: 0.157 |\tBpp loss: 0.10 |\tLog loss: 0.07 |\tAux loss: 22.89\n",
            "\n",
            "Train epoch 46: \tAcc: 97.018 |\n",
            "Test epoch 46: Average losses:\tAcc: 68.190 |\tLoss: 1.637 |\tBpp loss: 0.07 |\tLog loss: 2.11 |\tAux loss: 23.22\n",
            "\n",
            "Time: 63.5230\n",
            "Learning rate: 0.00025\n",
            "Train epoch 47: [0/50000 (0%)]\tLoss: 0.380 |\tBpp loss: 0.10 |\tLog loss: 0.32 |\tAux loss: 23.45\n",
            "Train epoch 47: [6400/50000 (13%)]\tLoss: 0.179 |\tBpp loss: 0.10 |\tLog loss: 0.09 |\tAux loss: 24.49\n",
            "Train epoch 47: [12800/50000 (26%)]\tLoss: 0.137 |\tBpp loss: 0.10 |\tLog loss: 0.04 |\tAux loss: 22.53\n",
            "Train epoch 47: [19200/50000 (38%)]\tLoss: 0.163 |\tBpp loss: 0.10 |\tLog loss: 0.07 |\tAux loss: 24.30\n",
            "Train epoch 47: [25600/50000 (51%)]\tLoss: 0.157 |\tBpp loss: 0.10 |\tLog loss: 0.07 |\tAux loss: 24.48\n",
            "Train epoch 47: [32000/50000 (64%)]\tLoss: 0.242 |\tBpp loss: 0.10 |\tLog loss: 0.16 |\tAux loss: 21.84\n",
            "Train epoch 47: [38400/50000 (77%)]\tLoss: 0.248 |\tBpp loss: 0.10 |\tLog loss: 0.17 |\tAux loss: 24.78\n",
            "Train epoch 47: [44800/50000 (90%)]\tLoss: 0.159 |\tBpp loss: 0.10 |\tLog loss: 0.07 |\tAux loss: 24.77\n",
            "\n",
            "Train epoch 47: \tAcc: 97.062 |\n",
            "Test epoch 47: Average losses:\tAcc: 68.240 |\tLoss: 1.681 |\tBpp loss: 0.07 |\tLog loss: 2.35 |\tAux loss: 26.26\n",
            "\n",
            "Time: 63.0274\n",
            "Learning rate: 0.00025\n",
            "Train epoch 48: [0/50000 (0%)]\tLoss: 0.240 |\tBpp loss: 0.10 |\tLog loss: 0.16 |\tAux loss: 26.29\n",
            "Train epoch 48: [6400/50000 (13%)]\tLoss: 0.281 |\tBpp loss: 0.10 |\tLog loss: 0.20 |\tAux loss: 22.76\n",
            "Train epoch 48: [12800/50000 (26%)]\tLoss: 0.217 |\tBpp loss: 0.10 |\tLog loss: 0.14 |\tAux loss: 23.47\n",
            "Train epoch 48: [19200/50000 (38%)]\tLoss: 0.134 |\tBpp loss: 0.10 |\tLog loss: 0.04 |\tAux loss: 23.44\n",
            "Train epoch 48: [25600/50000 (51%)]\tLoss: 0.166 |\tBpp loss: 0.10 |\tLog loss: 0.07 |\tAux loss: 23.52\n",
            "Train epoch 48: [32000/50000 (64%)]\tLoss: 0.270 |\tBpp loss: 0.10 |\tLog loss: 0.19 |\tAux loss: 23.87\n",
            "Train epoch 48: [38400/50000 (77%)]\tLoss: 0.140 |\tBpp loss: 0.10 |\tLog loss: 0.05 |\tAux loss: 23.72\n",
            "Train epoch 48: [44800/50000 (90%)]\tLoss: 0.115 |\tBpp loss: 0.10 |\tLog loss: 0.02 |\tAux loss: 22.89\n",
            "\n",
            "Train epoch 48: \tAcc: 97.116 |\n",
            "Test epoch 48: Average losses:\tAcc: 67.970 |\tLoss: 1.644 |\tBpp loss: 0.07 |\tLog loss: 2.32 |\tAux loss: 24.66\n",
            "\n",
            "Time: 62.9958\n",
            "Learning rate: 0.00025\n",
            "Train epoch 49: [0/50000 (0%)]\tLoss: 0.243 |\tBpp loss: 0.10 |\tLog loss: 0.16 |\tAux loss: 24.78\n",
            "Train epoch 49: [6400/50000 (13%)]\tLoss: 0.120 |\tBpp loss: 0.10 |\tLog loss: 0.03 |\tAux loss: 24.08\n",
            "Train epoch 49: [12800/50000 (26%)]\tLoss: 0.130 |\tBpp loss: 0.10 |\tLog loss: 0.04 |\tAux loss: 25.13\n",
            "Train epoch 49: [19200/50000 (38%)]\tLoss: 0.210 |\tBpp loss: 0.10 |\tLog loss: 0.12 |\tAux loss: 23.86\n",
            "Train epoch 49: [25600/50000 (51%)]\tLoss: 0.102 |\tBpp loss: 0.10 |\tLog loss: 0.01 |\tAux loss: 23.73\n",
            "Train epoch 49: [32000/50000 (64%)]\tLoss: 0.109 |\tBpp loss: 0.10 |\tLog loss: 0.01 |\tAux loss: 23.28\n",
            "Train epoch 49: [38400/50000 (77%)]\tLoss: 0.166 |\tBpp loss: 0.10 |\tLog loss: 0.07 |\tAux loss: 23.37\n",
            "Train epoch 49: [44800/50000 (90%)]\tLoss: 0.151 |\tBpp loss: 0.10 |\tLog loss: 0.06 |\tAux loss: 24.00\n",
            "\n",
            "Train epoch 49: \tAcc: 97.416 |\n",
            "Test epoch 49: Average losses:\tAcc: 67.970 |\tLoss: 1.696 |\tBpp loss: 0.07 |\tLog loss: 2.35 |\tAux loss: 23.26\n",
            "\n",
            "Time: 62.5632\n",
            "Learning rate: 0.00025\n",
            "Train epoch 50: [0/50000 (0%)]\tLoss: 0.134 |\tBpp loss: 0.10 |\tLog loss: 0.04 |\tAux loss: 23.46\n",
            "Train epoch 50: [6400/50000 (13%)]\tLoss: 0.162 |\tBpp loss: 0.10 |\tLog loss: 0.07 |\tAux loss: 24.72\n",
            "Train epoch 50: [12800/50000 (26%)]\tLoss: 0.122 |\tBpp loss: 0.09 |\tLog loss: 0.03 |\tAux loss: 24.00\n",
            "Train epoch 50: [19200/50000 (38%)]\tLoss: 0.124 |\tBpp loss: 0.10 |\tLog loss: 0.03 |\tAux loss: 23.12\n",
            "Train epoch 50: [25600/50000 (51%)]\tLoss: 0.134 |\tBpp loss: 0.10 |\tLog loss: 0.04 |\tAux loss: 25.07\n",
            "Train epoch 50: [32000/50000 (64%)]\tLoss: 0.244 |\tBpp loss: 0.10 |\tLog loss: 0.16 |\tAux loss: 24.93\n",
            "Train epoch 50: [38400/50000 (77%)]\tLoss: 0.182 |\tBpp loss: 0.10 |\tLog loss: 0.09 |\tAux loss: 25.61\n",
            "Train epoch 50: [44800/50000 (90%)]\tLoss: 0.198 |\tBpp loss: 0.09 |\tLog loss: 0.11 |\tAux loss: 24.94\n",
            "\n",
            "Train epoch 50: \tAcc: 97.504 |\n",
            "Test epoch 50: Average losses:\tAcc: 67.970 |\tLoss: 1.720 |\tBpp loss: 0.07 |\tLog loss: 2.24 |\tAux loss: 24.14\n",
            "\n",
            "Time: 63.0794\n",
            "Learning rate: 0.00025\n",
            "Train epoch 51: [0/50000 (0%)]\tLoss: 0.142 |\tBpp loss: 0.10 |\tLog loss: 0.05 |\tAux loss: 24.21\n",
            "Train epoch 51: [6400/50000 (13%)]\tLoss: 0.197 |\tBpp loss: 0.09 |\tLog loss: 0.11 |\tAux loss: 25.23\n",
            "Train epoch 51: [12800/50000 (26%)]\tLoss: 0.106 |\tBpp loss: 0.10 |\tLog loss: 0.01 |\tAux loss: 25.62\n",
            "Train epoch 51: [19200/50000 (38%)]\tLoss: 0.125 |\tBpp loss: 0.09 |\tLog loss: 0.03 |\tAux loss: 27.72\n",
            "Train epoch 51: [25600/50000 (51%)]\tLoss: 0.138 |\tBpp loss: 0.10 |\tLog loss: 0.05 |\tAux loss: 26.88\n",
            "Train epoch 51: [32000/50000 (64%)]\tLoss: 0.101 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 24.79\n",
            "Train epoch 51: [38400/50000 (77%)]\tLoss: 0.153 |\tBpp loss: 0.09 |\tLog loss: 0.07 |\tAux loss: 25.04\n",
            "Train epoch 51: [44800/50000 (90%)]\tLoss: 0.147 |\tBpp loss: 0.10 |\tLog loss: 0.06 |\tAux loss: 26.57\n",
            "\n",
            "Train epoch 51: \tAcc: 97.564 |\n",
            "Test epoch 51: Average losses:\tAcc: 67.850 |\tLoss: 1.757 |\tBpp loss: 0.07 |\tLog loss: 2.29 |\tAux loss: 26.22\n",
            "\n",
            "Time: 62.9930\n",
            "Learning rate: 0.00025\n",
            "Train epoch 52: [0/50000 (0%)]\tLoss: 0.192 |\tBpp loss: 0.09 |\tLog loss: 0.11 |\tAux loss: 26.43\n",
            "Train epoch 52: [6400/50000 (13%)]\tLoss: 0.118 |\tBpp loss: 0.10 |\tLog loss: 0.02 |\tAux loss: 23.79\n",
            "Train epoch 52: [12800/50000 (26%)]\tLoss: 0.134 |\tBpp loss: 0.09 |\tLog loss: 0.04 |\tAux loss: 25.14\n",
            "Train epoch 52: [19200/50000 (38%)]\tLoss: 0.140 |\tBpp loss: 0.10 |\tLog loss: 0.05 |\tAux loss: 25.05\n",
            "Train epoch 52: [25600/50000 (51%)]\tLoss: 0.198 |\tBpp loss: 0.09 |\tLog loss: 0.12 |\tAux loss: 24.41\n",
            "Train epoch 52: [32000/50000 (64%)]\tLoss: 0.146 |\tBpp loss: 0.10 |\tLog loss: 0.05 |\tAux loss: 27.81\n",
            "Train epoch 52: [38400/50000 (77%)]\tLoss: 0.177 |\tBpp loss: 0.10 |\tLog loss: 0.08 |\tAux loss: 25.55\n",
            "Train epoch 52: [44800/50000 (90%)]\tLoss: 0.132 |\tBpp loss: 0.09 |\tLog loss: 0.04 |\tAux loss: 26.28\n",
            "\n",
            "Train epoch 52: \tAcc: 97.834 |\n",
            "Test epoch 52: Average losses:\tAcc: 67.430 |\tLoss: 1.793 |\tBpp loss: 0.07 |\tLog loss: 2.08 |\tAux loss: 24.82\n",
            "\n",
            "Time: 62.6927\n",
            "Learning rate: 0.00025\n",
            "Train epoch 53: [0/50000 (0%)]\tLoss: 0.105 |\tBpp loss: 0.10 |\tLog loss: 0.01 |\tAux loss: 24.77\n",
            "Train epoch 53: [6400/50000 (13%)]\tLoss: 0.120 |\tBpp loss: 0.10 |\tLog loss: 0.02 |\tAux loss: 23.34\n",
            "Train epoch 53: [12800/50000 (26%)]\tLoss: 0.169 |\tBpp loss: 0.10 |\tLog loss: 0.08 |\tAux loss: 24.35\n",
            "Train epoch 53: [19200/50000 (38%)]\tLoss: 0.184 |\tBpp loss: 0.09 |\tLog loss: 0.10 |\tAux loss: 25.19\n",
            "Train epoch 53: [25600/50000 (51%)]\tLoss: 0.123 |\tBpp loss: 0.10 |\tLog loss: 0.03 |\tAux loss: 25.36\n",
            "Train epoch 53: [32000/50000 (64%)]\tLoss: 0.227 |\tBpp loss: 0.09 |\tLog loss: 0.15 |\tAux loss: 24.27\n",
            "Train epoch 53: [38400/50000 (77%)]\tLoss: 0.171 |\tBpp loss: 0.10 |\tLog loss: 0.08 |\tAux loss: 25.71\n",
            "Train epoch 53: [44800/50000 (90%)]\tLoss: 0.159 |\tBpp loss: 0.09 |\tLog loss: 0.07 |\tAux loss: 24.78\n",
            "\n",
            "Train epoch 53: \tAcc: 97.788 |\n",
            "Test epoch 53: Average losses:\tAcc: 67.770 |\tLoss: 1.813 |\tBpp loss: 0.07 |\tLog loss: 2.18 |\tAux loss: 24.30\n",
            "\n",
            "Time: 62.5322\n",
            "Learning rate: 0.00025\n",
            "Train epoch 54: [0/50000 (0%)]\tLoss: 0.209 |\tBpp loss: 0.09 |\tLog loss: 0.13 |\tAux loss: 24.34\n",
            "Train epoch 54: [6400/50000 (13%)]\tLoss: 0.117 |\tBpp loss: 0.09 |\tLog loss: 0.03 |\tAux loss: 24.59\n",
            "Train epoch 54: [12800/50000 (26%)]\tLoss: 0.229 |\tBpp loss: 0.10 |\tLog loss: 0.15 |\tAux loss: 23.78\n",
            "Train epoch 54: [19200/50000 (38%)]\tLoss: 0.112 |\tBpp loss: 0.10 |\tLog loss: 0.02 |\tAux loss: 23.66\n",
            "Train epoch 54: [25600/50000 (51%)]\tLoss: 0.148 |\tBpp loss: 0.10 |\tLog loss: 0.06 |\tAux loss: 25.63\n",
            "Train epoch 54: [32000/50000 (64%)]\tLoss: 0.120 |\tBpp loss: 0.10 |\tLog loss: 0.03 |\tAux loss: 24.80\n",
            "Train epoch 54: [38400/50000 (77%)]\tLoss: 0.307 |\tBpp loss: 0.10 |\tLog loss: 0.23 |\tAux loss: 25.27\n",
            "Train epoch 54: [44800/50000 (90%)]\tLoss: 0.152 |\tBpp loss: 0.09 |\tLog loss: 0.07 |\tAux loss: 25.50\n",
            "\n",
            "Train epoch 54: \tAcc: 97.884 |\n",
            "Test epoch 54: Average losses:\tAcc: 67.460 |\tLoss: 1.879 |\tBpp loss: 0.07 |\tLog loss: 2.37 |\tAux loss: 24.85\n",
            "\n",
            "Time: 62.4841\n",
            "Learning rate: 0.00025\n",
            "Train epoch 55: [0/50000 (0%)]\tLoss: 0.103 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 25.05\n",
            "Train epoch 55: [6400/50000 (13%)]\tLoss: 0.260 |\tBpp loss: 0.10 |\tLog loss: 0.18 |\tAux loss: 27.51\n",
            "Train epoch 55: [12800/50000 (26%)]\tLoss: 0.136 |\tBpp loss: 0.09 |\tLog loss: 0.05 |\tAux loss: 24.41\n",
            "Train epoch 55: [19200/50000 (38%)]\tLoss: 0.108 |\tBpp loss: 0.09 |\tLog loss: 0.02 |\tAux loss: 25.39\n",
            "Train epoch 55: [25600/50000 (51%)]\tLoss: 0.146 |\tBpp loss: 0.10 |\tLog loss: 0.05 |\tAux loss: 24.49\n",
            "Train epoch 55: [32000/50000 (64%)]\tLoss: 0.143 |\tBpp loss: 0.10 |\tLog loss: 0.05 |\tAux loss: 25.20\n",
            "Train epoch 55: [38400/50000 (77%)]\tLoss: 0.126 |\tBpp loss: 0.10 |\tLog loss: 0.03 |\tAux loss: 28.04\n",
            "Train epoch 55: [44800/50000 (90%)]\tLoss: 0.205 |\tBpp loss: 0.09 |\tLog loss: 0.12 |\tAux loss: 25.27\n",
            "\n",
            "Train epoch 55: \tAcc: 97.996 |\n",
            "Test epoch 55: Average losses:\tAcc: 67.780 |\tLoss: 1.841 |\tBpp loss: 0.06 |\tLog loss: 2.46 |\tAux loss: 25.53\n",
            "\n",
            "Time: 62.8520\n",
            "Learning rate: 0.000125\n",
            "Train epoch 56: [0/50000 (0%)]\tLoss: 0.140 |\tBpp loss: 0.09 |\tLog loss: 0.05 |\tAux loss: 25.65\n",
            "Train epoch 56: [6400/50000 (13%)]\tLoss: 0.107 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 24.57\n",
            "Train epoch 56: [12800/50000 (26%)]\tLoss: 0.105 |\tBpp loss: 0.10 |\tLog loss: 0.01 |\tAux loss: 24.84\n",
            "Train epoch 56: [19200/50000 (38%)]\tLoss: 0.203 |\tBpp loss: 0.09 |\tLog loss: 0.12 |\tAux loss: 24.92\n",
            "Train epoch 56: [25600/50000 (51%)]\tLoss: 0.106 |\tBpp loss: 0.09 |\tLog loss: 0.02 |\tAux loss: 24.19\n",
            "Train epoch 56: [32000/50000 (64%)]\tLoss: 0.105 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 25.14\n",
            "Train epoch 56: [38400/50000 (77%)]\tLoss: 0.111 |\tBpp loss: 0.09 |\tLog loss: 0.02 |\tAux loss: 26.60\n",
            "Train epoch 56: [44800/50000 (90%)]\tLoss: 0.101 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 27.26\n",
            "\n",
            "Train epoch 56: \tAcc: 98.706 |\n",
            "Test epoch 56: Average losses:\tAcc: 68.050 |\tLoss: 1.923 |\tBpp loss: 0.06 |\tLog loss: 2.44 |\tAux loss: 27.88\n",
            "\n",
            "Time: 62.3901\n",
            "Learning rate: 0.000125\n",
            "Train epoch 57: [0/50000 (0%)]\tLoss: 0.104 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 27.96\n",
            "Train epoch 57: [6400/50000 (13%)]\tLoss: 0.102 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 25.89\n",
            "Train epoch 57: [12800/50000 (26%)]\tLoss: 0.202 |\tBpp loss: 0.09 |\tLog loss: 0.12 |\tAux loss: 27.17\n",
            "Train epoch 57: [19200/50000 (38%)]\tLoss: 0.096 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 28.98\n",
            "Train epoch 57: [25600/50000 (51%)]\tLoss: 0.147 |\tBpp loss: 0.09 |\tLog loss: 0.06 |\tAux loss: 27.68\n",
            "Train epoch 57: [32000/50000 (64%)]\tLoss: 0.179 |\tBpp loss: 0.09 |\tLog loss: 0.09 |\tAux loss: 27.50\n",
            "Train epoch 57: [38400/50000 (77%)]\tLoss: 0.123 |\tBpp loss: 0.09 |\tLog loss: 0.04 |\tAux loss: 27.49\n",
            "Train epoch 57: [44800/50000 (90%)]\tLoss: 0.099 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 29.74\n",
            "\n",
            "Train epoch 57: \tAcc: 98.760 |\n",
            "Test epoch 57: Average losses:\tAcc: 68.020 |\tLoss: 2.021 |\tBpp loss: 0.06 |\tLog loss: 2.56 |\tAux loss: 29.97\n",
            "\n",
            "Time: 63.1834\n",
            "Learning rate: 0.000125\n",
            "Train epoch 58: [0/50000 (0%)]\tLoss: 0.110 |\tBpp loss: 0.09 |\tLog loss: 0.02 |\tAux loss: 29.94\n",
            "Train epoch 58: [6400/50000 (13%)]\tLoss: 0.095 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 26.96\n",
            "Train epoch 58: [12800/50000 (26%)]\tLoss: 0.143 |\tBpp loss: 0.09 |\tLog loss: 0.06 |\tAux loss: 29.60\n",
            "Train epoch 58: [19200/50000 (38%)]\tLoss: 0.146 |\tBpp loss: 0.09 |\tLog loss: 0.06 |\tAux loss: 30.60\n",
            "Train epoch 58: [25600/50000 (51%)]\tLoss: 0.221 |\tBpp loss: 0.09 |\tLog loss: 0.14 |\tAux loss: 29.56\n",
            "Train epoch 58: [32000/50000 (64%)]\tLoss: 0.174 |\tBpp loss: 0.09 |\tLog loss: 0.09 |\tAux loss: 30.82\n",
            "Train epoch 58: [38400/50000 (77%)]\tLoss: 0.114 |\tBpp loss: 0.09 |\tLog loss: 0.03 |\tAux loss: 28.81\n",
            "Train epoch 58: [44800/50000 (90%)]\tLoss: 0.104 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 31.25\n",
            "\n",
            "Train epoch 58: \tAcc: 98.750 |\n",
            "Test epoch 58: Average losses:\tAcc: 67.430 |\tLoss: 2.125 |\tBpp loss: 0.06 |\tLog loss: 2.69 |\tAux loss: 29.62\n",
            "\n",
            "Time: 62.8053\n",
            "Learning rate: 0.000125\n",
            "Train epoch 59: [0/50000 (0%)]\tLoss: 0.110 |\tBpp loss: 0.09 |\tLog loss: 0.02 |\tAux loss: 29.56\n",
            "Train epoch 59: [6400/50000 (13%)]\tLoss: 0.100 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 29.85\n",
            "Train epoch 59: [12800/50000 (26%)]\tLoss: 0.119 |\tBpp loss: 0.09 |\tLog loss: 0.03 |\tAux loss: 29.74\n",
            "Train epoch 59: [19200/50000 (38%)]\tLoss: 0.151 |\tBpp loss: 0.09 |\tLog loss: 0.07 |\tAux loss: 33.02\n",
            "Train epoch 59: [25600/50000 (51%)]\tLoss: 0.141 |\tBpp loss: 0.09 |\tLog loss: 0.05 |\tAux loss: 30.07\n",
            "Train epoch 59: [32000/50000 (64%)]\tLoss: 0.118 |\tBpp loss: 0.09 |\tLog loss: 0.03 |\tAux loss: 29.61\n",
            "Train epoch 59: [38400/50000 (77%)]\tLoss: 0.115 |\tBpp loss: 0.09 |\tLog loss: 0.03 |\tAux loss: 32.85\n",
            "Train epoch 59: [44800/50000 (90%)]\tLoss: 0.124 |\tBpp loss: 0.09 |\tLog loss: 0.04 |\tAux loss: 32.57\n",
            "\n",
            "Train epoch 59: \tAcc: 98.862 |\n",
            "Test epoch 59: Average losses:\tAcc: 68.120 |\tLoss: 2.162 |\tBpp loss: 0.06 |\tLog loss: 2.93 |\tAux loss: 30.42\n",
            "\n",
            "Time: 63.4300\n",
            "Learning rate: 0.000125\n",
            "Train epoch 60: [0/50000 (0%)]\tLoss: 0.232 |\tBpp loss: 0.09 |\tLog loss: 0.16 |\tAux loss: 30.50\n",
            "Train epoch 60: [6400/50000 (13%)]\tLoss: 0.129 |\tBpp loss: 0.09 |\tLog loss: 0.05 |\tAux loss: 31.11\n",
            "Train epoch 60: [12800/50000 (26%)]\tLoss: 0.103 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 30.95\n",
            "Train epoch 60: [19200/50000 (38%)]\tLoss: 0.144 |\tBpp loss: 0.09 |\tLog loss: 0.06 |\tAux loss: 32.17\n",
            "Train epoch 60: [25600/50000 (51%)]\tLoss: 0.126 |\tBpp loss: 0.09 |\tLog loss: 0.04 |\tAux loss: 31.67\n",
            "Train epoch 60: [32000/50000 (64%)]\tLoss: 0.105 |\tBpp loss: 0.09 |\tLog loss: 0.02 |\tAux loss: 31.07\n",
            "Train epoch 60: [38400/50000 (77%)]\tLoss: 0.189 |\tBpp loss: 0.09 |\tLog loss: 0.11 |\tAux loss: 32.85\n",
            "Train epoch 60: [44800/50000 (90%)]\tLoss: 0.093 |\tBpp loss: 0.09 |\tLog loss: 0.00 |\tAux loss: 33.36\n",
            "\n",
            "Train epoch 60: \tAcc: 98.864 |\n",
            "Test epoch 60: Average losses:\tAcc: 67.860 |\tLoss: 2.166 |\tBpp loss: 0.06 |\tLog loss: 2.71 |\tAux loss: 31.37\n",
            "\n",
            "Time: 63.3614\n",
            "Learning rate: 0.000125\n",
            "Train epoch 61: [0/50000 (0%)]\tLoss: 0.122 |\tBpp loss: 0.09 |\tLog loss: 0.03 |\tAux loss: 31.39\n",
            "Train epoch 61: [6400/50000 (13%)]\tLoss: 0.094 |\tBpp loss: 0.09 |\tLog loss: 0.00 |\tAux loss: 33.77\n",
            "Train epoch 61: [12800/50000 (26%)]\tLoss: 0.144 |\tBpp loss: 0.09 |\tLog loss: 0.06 |\tAux loss: 32.20\n",
            "Train epoch 61: [19200/50000 (38%)]\tLoss: 0.220 |\tBpp loss: 0.09 |\tLog loss: 0.15 |\tAux loss: 33.14\n",
            "Train epoch 61: [25600/50000 (51%)]\tLoss: 0.095 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 31.61\n",
            "Train epoch 61: [32000/50000 (64%)]\tLoss: 0.106 |\tBpp loss: 0.09 |\tLog loss: 0.02 |\tAux loss: 32.22\n",
            "Train epoch 61: [38400/50000 (77%)]\tLoss: 0.094 |\tBpp loss: 0.09 |\tLog loss: 0.00 |\tAux loss: 32.24\n",
            "Train epoch 61: [44800/50000 (90%)]\tLoss: 0.097 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 33.32\n",
            "\n",
            "Train epoch 61: \tAcc: 98.908 |\n",
            "Test epoch 61: Average losses:\tAcc: 67.790 |\tLoss: 2.193 |\tBpp loss: 0.06 |\tLog loss: 3.01 |\tAux loss: 30.24\n",
            "\n",
            "Time: 62.6042\n",
            "Learning rate: 0.000125\n",
            "Train epoch 62: [0/50000 (0%)]\tLoss: 0.134 |\tBpp loss: 0.09 |\tLog loss: 0.05 |\tAux loss: 30.27\n",
            "Train epoch 62: [6400/50000 (13%)]\tLoss: 0.100 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 33.50\n",
            "Train epoch 62: [12800/50000 (26%)]\tLoss: 0.169 |\tBpp loss: 0.09 |\tLog loss: 0.09 |\tAux loss: 32.04\n",
            "Train epoch 62: [19200/50000 (38%)]\tLoss: 0.105 |\tBpp loss: 0.09 |\tLog loss: 0.02 |\tAux loss: 33.72\n",
            "Train epoch 62: [25600/50000 (51%)]\tLoss: 0.162 |\tBpp loss: 0.09 |\tLog loss: 0.08 |\tAux loss: 32.57\n",
            "Train epoch 62: [32000/50000 (64%)]\tLoss: 0.141 |\tBpp loss: 0.09 |\tLog loss: 0.06 |\tAux loss: 33.18\n",
            "Train epoch 62: [38400/50000 (77%)]\tLoss: 0.098 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 34.06\n",
            "Train epoch 62: [44800/50000 (90%)]\tLoss: 0.111 |\tBpp loss: 0.09 |\tLog loss: 0.02 |\tAux loss: 32.40\n",
            "\n",
            "Train epoch 62: \tAcc: 98.932 |\n",
            "Test epoch 62: Average losses:\tAcc: 67.820 |\tLoss: 2.231 |\tBpp loss: 0.06 |\tLog loss: 2.92 |\tAux loss: 32.51\n",
            "\n",
            "Time: 62.6962\n",
            "Learning rate: 0.000125\n",
            "Train epoch 63: [0/50000 (0%)]\tLoss: 0.130 |\tBpp loss: 0.09 |\tLog loss: 0.04 |\tAux loss: 32.61\n",
            "Train epoch 63: [6400/50000 (13%)]\tLoss: 0.126 |\tBpp loss: 0.09 |\tLog loss: 0.04 |\tAux loss: 34.65\n",
            "Train epoch 63: [12800/50000 (26%)]\tLoss: 0.090 |\tBpp loss: 0.09 |\tLog loss: 0.00 |\tAux loss: 32.65\n",
            "Train epoch 63: [19200/50000 (38%)]\tLoss: 0.107 |\tBpp loss: 0.09 |\tLog loss: 0.02 |\tAux loss: 33.25\n",
            "Train epoch 63: [25600/50000 (51%)]\tLoss: 0.121 |\tBpp loss: 0.09 |\tLog loss: 0.03 |\tAux loss: 32.48\n",
            "Train epoch 63: [32000/50000 (64%)]\tLoss: 0.115 |\tBpp loss: 0.09 |\tLog loss: 0.03 |\tAux loss: 33.83\n",
            "Train epoch 63: [38400/50000 (77%)]\tLoss: 0.125 |\tBpp loss: 0.09 |\tLog loss: 0.04 |\tAux loss: 32.40\n",
            "Train epoch 63: [44800/50000 (90%)]\tLoss: 0.093 |\tBpp loss: 0.09 |\tLog loss: 0.00 |\tAux loss: 34.94\n",
            "\n",
            "Train epoch 63: \tAcc: 98.986 |\n",
            "Test epoch 63: Average losses:\tAcc: 68.140 |\tLoss: 2.229 |\tBpp loss: 0.06 |\tLog loss: 2.61 |\tAux loss: 32.73\n",
            "\n",
            "Time: 62.1016\n",
            "Learning rate: 0.000125\n",
            "Train epoch 64: [0/50000 (0%)]\tLoss: 0.103 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 32.76\n",
            "Train epoch 64: [6400/50000 (13%)]\tLoss: 0.138 |\tBpp loss: 0.09 |\tLog loss: 0.05 |\tAux loss: 32.59\n",
            "Train epoch 64: [12800/50000 (26%)]\tLoss: 0.101 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 33.90\n",
            "Train epoch 64: [19200/50000 (38%)]\tLoss: 0.096 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 33.57\n",
            "Train epoch 64: [25600/50000 (51%)]\tLoss: 0.096 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 32.97\n",
            "Train epoch 64: [32000/50000 (64%)]\tLoss: 0.090 |\tBpp loss: 0.09 |\tLog loss: 0.00 |\tAux loss: 31.76\n",
            "Train epoch 64: [38400/50000 (77%)]\tLoss: 0.102 |\tBpp loss: 0.09 |\tLog loss: 0.02 |\tAux loss: 36.64\n",
            "Train epoch 64: [44800/50000 (90%)]\tLoss: 0.167 |\tBpp loss: 0.09 |\tLog loss: 0.09 |\tAux loss: 34.02\n",
            "\n",
            "Train epoch 64: \tAcc: 99.008 |\n",
            "Test epoch 64: Average losses:\tAcc: 67.920 |\tLoss: 2.261 |\tBpp loss: 0.06 |\tLog loss: 2.77 |\tAux loss: 33.92\n",
            "\n",
            "Time: 62.6034\n",
            "Learning rate: 0.000125\n",
            "Train epoch 65: [0/50000 (0%)]\tLoss: 0.115 |\tBpp loss: 0.09 |\tLog loss: 0.03 |\tAux loss: 33.95\n",
            "Train epoch 65: [6400/50000 (13%)]\tLoss: 0.095 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 35.73\n",
            "Train epoch 65: [12800/50000 (26%)]\tLoss: 0.099 |\tBpp loss: 0.09 |\tLog loss: 0.01 |\tAux loss: 34.73\n",
            "Train epoch 65: [19200/50000 (38%)]\tLoss: 0.111 |\tBpp loss: 0.09 |\tLog loss: 0.02 |\tAux loss: 33.78\n",
            "Train epoch 65: [25600/50000 (51%)]\tLoss: 0.114 |\tBpp loss: 0.09 |\tLog loss: 0.03 |\tAux loss: 34.13\n",
            "Train epoch 65: [32000/50000 (64%)]\tLoss: 0.109 |\tBpp loss: 0.09 |\tLog loss: 0.03 |\tAux loss: 34.32\n",
            "Train epoch 65: [38400/50000 (77%)]\tLoss: 0.109 |\tBpp loss: 0.09 |\tLog loss: 0.02 |\tAux loss: 34.10\n",
            "Train epoch 65: [44800/50000 (90%)]\tLoss: 0.142 |\tBpp loss: 0.09 |\tLog loss: 0.06 |\tAux loss: 34.71\n",
            "\n",
            "Train epoch 65: \tAcc: 99.010 |\n"
          ]
        }
      ]
    }
  ]
}